model_dir: /mnt/beegfs/home/pham/multi-domain-nmt/paraphrase/models/sparse_src_masking_30

domain_numb: 3

training_label_file: /mnt/beegfs/home/pham/multi-domain-nmt/sparse/data/training_corpora/emea_epps_ecb/all.en_fr.en.bpe.tok
       
training_feature_file: /mnt/beegfs/home/pham/multi-domain-nmt/sparse/data/training_corpora/emea_epps_ecb/all.en_fr.fr.bpe.tok
      
training_tag_file: /mnt/beegfs/home/pham/multi-domain-nmt/sparse/data/training_corpora/emea_epps_ecb/all.en_fr.en.bpe.tok.prob.tag.0
     
eval_feature_file:
        - /mnt/beegfs/home/pham/multi-domain-nmt/sparse/data/valid_corpora/emea/EMEA.en-fr.en.bpe.tok.dev
        - /mnt/beegfs/home/pham/multi-domain-nmt/sparse/data/valid_corpora/epps/europarl-v7.fr-en.en.bpe.tok.dev
        - /mnt/beegfs/home/pham/multi-domain-nmt/sparse/data/valid_corpora/ecb/ECB.en-fr.en.dev.bpe.tok
eval_label_file:
        - /mnt/beegfs/home/pham/multi-domain-nmt/sparse/data/valid_corpora/emea/EMEA.en-fr.fr.bpe.tok.dev
        - /mnt/beegfs/home/pham/multi-domain-nmt/sparse/data/valid_corpora/epps/europarl-v7.fr-en.fr.bpe.tok.dev
        - /mnt/beegfs/home/pham/multi-domain-nmt/sparse/data/valid_corpora/ecb/ECB.en-fr.fr.dev.bpe.tok
eval_tag_file:
        - /mnt/beegfs/home/pham/multi-domain-nmt/sparse/data/valid_corpora/emea/EMEA.en-fr.en.bpe.tok.dev.tag.1
        - /mnt/beegfs/home/pham/multi-domain-nmt/sparse/data/valid_corpora/epps/europarl-v7.fr-en.en.bpe.tok.dev.tag.1
        - /mnt/beegfs/home/pham/multi-domain-nmt/sparse/data/valid_corpora/ecb/ECB.en-fr.en.dev.bpe.tok.tag.1

Architecture: Transformer

src_vocab_path: /mnt/beegfs/home/pham/multi-domain-nmt/sparse/data/vocab/vocab.en_fr.en

tgt_vocab_path: /mnt/beegfs/home/pham/multi-domain-nmt/sparse/data/vocab/vocab.en_fr.fr

src_vocab_size: 30164

tgt_vocab_size: 30398

optimizer_parameters:
        optimizer: LazyAdamOptimizer #GradientDescentOptimizer
        learning_rate: 1.0 # The scale constant.
        decay_type: noam_decay_v2
        decay_params:
            model_dim: 512
            warmup_steps: 4000
        decay_step_duration: 8
        start_decay_steps: 0
        clip_gradients: Null
        gradients_accum: 1
        
#OPTIMIZER_CLS_NAMES = {
#    "Adagrad": train.AdagradOptimizer,
#    "Adam": train.AdamOptimizer,
#    "Ftrl": train.FtrlOptimizer,
#    "Momentum": lambda learning_rate: train.MomentumOptimizer(learning_rate, momentum=0.9),  # pylint: disable=line-too-long
#    "RMSProp": train.RMSPropOptimizer,
#    "SGD": train.GradientDescentOptimizer,
#    }

mode: Training

verage_loss_in_time: true
label_smoothing: 0.1
beam_width: 5
length_penalty: 0.6

num_devices: 2
num_threads: 20

daisy_chain_variables: true

iteration_number: 200000

training_batch_type: tokens

example_sampling_distribution: Natural

dataprocess_version: 

training_batch_size: 3072

eval_batch_size: 120

max_len: 80

Standard: true
position_mask: false
Fusion_layer: true
generic_batch: true
projector_masking: false
src_masking: true
tgt_masking: true
Generic_region_adversarial_training: false
src_adv_training: false
tgt_adv_training: false
embedding_adv_training: false
encoder_adv_training: false

discriminator_optimizer_parameters:
        optimizer: LazyAdamOptimizer
        learning_rate: 0.002 # The scale constant.                
        decay_type: noam_decay_v2
        decay_params:
            model_dim: 512
            warmup_steps: 4000
        decay_step_duration: 8
        start_decay_steps: 0
        gradients_accum: 1
        clip_gradients: Null

dis_training_step: 2
dis_step: 10000
coeff_increasing_interval: 100000
lambda_E: 0.01

src_sharing_embedding_region_size: 488

src_domain_embedding_region_size: 
        - 8
        - 8
        - 8
tgt_embedding_size: 512

hidden_size: 512

save_freq: 5000

printing_freq: 100

eval_freq: 15000

summary_freq: 200

max_to_keep : 30

sample_buffer_size : 22000000
