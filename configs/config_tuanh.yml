model_dir: models/try

training_label_file: /home/tnguyen/datasets/Quora/Quora-50k/text_1_train.txt
       
training_feature_file: /home/tnguyen/datasets/Quora/Quora-50k/text_2_train.txt
     
eval_feature_file:
        - /home/tnguyen/datasets/Quora/Quora-50k/text_1_test.txt
eval_label_file:
        - /home/tnguyen/datasets/Quora/Quora-50k/text_2_test.txt

Architecture: Transformer

src_vocab_path: /home/tnguyen/datasets/Quora/Quora-50k/text_1_vocab.txt

tgt_vocab_path: /home/tnguyen/datasets/Quora/Quora-50k/text_2_vocab.txt

src_vocab_size: 20000

tgt_vocab_size: 20000

optimizer_parameters:
        optimizer: LazyAdamOptimizer #GradientDescentOptimizer
        learning_rate: 1.0 # The scale constant.
        decay_type: noam_decay_v2
        decay_params:
            model_dim: 512
            warmup_steps: 4000
        decay_step_duration: 8
        start_decay_steps: 0
        clip_gradients: Null
        gradients_accum: 1
        
#OPTIMIZER_CLS_NAMES = {
#    "Adagrad": train.AdagradOptimizer,
#    "Adam": train.AdamOptimizer,
#    "Ftrl": train.FtrlOptimizer,
#    "Momentum": lambda learning_rate: train.MomentumOptimizer(learning_rate, momentum=0.9),  # pylint: disable=line-too-long
#    "RMSProp": train.RMSPropOptimizer,
#    "SGD": train.GradientDescentOptimizer,
#    }

mode: Training

verage_loss_in_time: true
label_smoothing: 0.1
beam_width: 5
length_penalty: 0.6

num_devices: 2
num_threads: 20

daisy_chain_variables: true

iteration_number: 200000

training_batch_type: tokens

example_sampling_distribution: Natural

dataprocess_version: 

training_batch_size: 3072

eval_batch_size: 120

max_len: 80

Standard: true
position_mask: false
Fusion_layer: true
generic_batch: true
projector_masking: false
src_masking: true
tgt_masking: true
Generic_region_adversarial_training: false
src_adv_training: false
tgt_adv_training: false
embedding_adv_training: false
encoder_adv_training: false

discriminator_optimizer_parameters:
        optimizer: LazyAdamOptimizer
        learning_rate: 0.002 # The scale constant.                
        decay_type: noam_decay_v2
        decay_params:
            model_dim: 512
            warmup_steps: 4000
        decay_step_duration: 8
        start_decay_steps: 0
        gradients_accum: 1
        clip_gradients: Null

dis_training_step: 2
dis_step: 10000
coeff_increasing_interval: 100000
lambda_E: 0.01

src_sharing_embedding_region_size: 488

src_domain_embedding_region_size: 
        - 8
        - 8
        - 8
tgt_embedding_size: 512

hidden_size: 512

save_freq: 1000

printing_freq: 100

eval_freq: 1000

summary_freq: 200

max_to_keep : 30

sample_buffer_size : 22000000
