{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from utils.dataprocess import *\n",
    "from utils.utils_ import *\n",
    "import argparse\n",
    "import sys\n",
    "import numpy as np\n",
    "from opennmt.inputters.text_inputter import load_pretrained_embeddings\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import ipdb\n",
    "import copy\n",
    "import yaml\n",
    "import io\n",
    "\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import common_shapes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.keras import activations\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras import constraints\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.layers.base import InputSpec\n",
    "from tensorflow.python.layers.base import Layer\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import gen_math_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import standard_ops\n",
    "from tensorflow.python.util.tf_export import tf_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import opennmt as onmt\n",
    "from opennmt.utils.misc import count_lines\n",
    "from opennmt.utils.parallel import GraphDispatcher\n",
    "from opennmt import constants\n",
    "from opennmt.utils.losses import cross_entropy_sequence_loss\n",
    "from opennmt.utils.optim import *\n",
    "from opennmt.utils.evaluator import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_size(data_file):\n",
    "    return count_lines(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(vocab_size, depth=512):\n",
    "      \"\"\"Creates an embedding variable.\"\"\"\n",
    "      return tf.get_variable(\"embedding\", shape = [vocab_size, depth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(vocab_path, vocab_size):\n",
    "    if not vocab_size:\n",
    "        vocab_size = count_lines(vocab_path) + 1 #for UNK\n",
    "    vocab = tf.contrib.lookup.index_table_from_file(vocab_path, vocab_size = vocab_size - 1, num_oov_buckets = 1)\n",
    "    return vocab, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_shapes(dataset):    \n",
    "    return tf.contrib.framework.nest.map_structure(\n",
    "    lambda shape: shape.as_list(), dataset.output_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_irregular_batches(multiple):    \n",
    "    if multiple == 1:\n",
    "        return lambda dataset: dataset\n",
    "\n",
    "    def _predicate(*x):\n",
    "        flat = tf.contrib.framework.nest.flatten(x)\n",
    "        batch_size = tf.shape(flat[0])[0]\n",
    "        return tf.equal(tf.mod(batch_size, multiple), 0)\n",
    "\n",
    "    return lambda dataset: dataset.filter(_predicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefetch_element(buffer_size=None):  \n",
    "    support_auto_tuning = hasattr(tf.data, \"experimental\") or hasattr(tf.contrib.data, \"AUTOTUNE\")\n",
    "    if not support_auto_tuning and buffer_size is None:\n",
    "        buffer_size = 1\n",
    "    return lambda dataset: dataset.prefetch(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(src_path, src_vocab, batch_size=32, batch_type =\"examples\", batch_multiplier = 1, tgt_path=None, tgt_vocab=None, \n",
    "              max_len=50, bucket_width = 1, mode=\"Training\", padded_shapes = None, \n",
    "              shuffle_buffer_size = None, prefetch_buffer_size = 100000, num_threads = 4, version=None, distribution=None, tf_idf_table=None):\n",
    "\n",
    "    batch_size = batch_size * batch_multiplier\n",
    "    print(\"batch_size\", batch_size)\n",
    "    \n",
    "    def _make_dataset(text_path):\n",
    "        dataset = tf.data.TextLineDataset(text_path)\n",
    "        dataset = dataset.map(lambda x: tf.string_split([x]).values) #split by spaces\n",
    "        return dataset    \n",
    "       \n",
    "    def _batch_func(dataset):\n",
    "        return dataset.padded_batch(batch_size,\n",
    "                                    padded_shapes=padded_shapes or get_padded_shapes(dataset))\n",
    "\n",
    "    def _key_func(dataset):                \n",
    "        #bucket_id = tf.squeeze(dataset[\"domain\"])\n",
    "        features_length = dataset[\"src_length\"] #features_length_fn(features) if features_length_fn is not None else None\n",
    "        labels_length = dataset[\"tgt_length\"] #labels_length_fn(labels) if labels_length_fn is not None else None        \n",
    "        bucket_id = tf.constant(0, dtype=tf.int32)\n",
    "        if features_length is not None:\n",
    "            bucket_id = tf.maximum(bucket_id, features_length // bucket_width)\n",
    "        if labels_length is not None:\n",
    "            bucket_id = tf.maximum(bucket_id, labels_length // bucket_width)\n",
    "        return tf.cast(bucket_id, tf.int64)\n",
    "        #return tf.to_int64(bucket_id)\n",
    "\n",
    "    def _reduce_func(unused_key, dataset):\n",
    "        return _batch_func(dataset)\n",
    "\n",
    "    def _window_size_func(key):\n",
    "        if bucket_width > 1:\n",
    "            key += 1  # For bucket_width == 1, key 0 is unassigned.\n",
    "        size = batch_size // (key * bucket_width)\n",
    "        if batch_multiplier > 1:\n",
    "            # Make the window size a multiple of batch_multiplier.\n",
    "            size = size + batch_multiplier - size % batch_multiplier\n",
    "        return tf.to_int64(tf.maximum(size, batch_multiplier))             \n",
    "    \n",
    "    bos = tf.constant([constants.START_OF_SENTENCE_ID], dtype=tf.int64)\n",
    "    eos = tf.constant([constants.END_OF_SENTENCE_ID], dtype=tf.int64)\n",
    "    \n",
    "    if version==None:\n",
    "        print(\"old dataprocessing version\")\n",
    "        src_dataset = _make_dataset(src_path)            \n",
    "        if mode==\"Training\":\n",
    "            tgt_dataset = _make_dataset(tgt_path)\n",
    "            dataset = tf.data.Dataset.zip((src_dataset, tgt_dataset))\n",
    "        elif mode==\"Inference\":\n",
    "            dataset = src_dataset\n",
    "        elif mode == \"Predict\":\n",
    "            dataset = src_dataset\n",
    "\n",
    "        if mode==\"Training\":                    \n",
    "            dataset = dataset.map(lambda x,y:{                      \n",
    "                    \"src_raw\": x,\n",
    "                    \"tgt_raw\": y,\n",
    "                    \"src_ids\": src_vocab.lookup(x),\n",
    "                    \"tgt_ids\": tgt_vocab.lookup(y),\n",
    "                    \"tgt_ids_in\": tf.concat([bos, tgt_vocab.lookup(y)], axis=0),\n",
    "                    \"tgt_ids_out\": tf.concat([tgt_vocab.lookup(y), eos], axis=0),\n",
    "                    \"src_length\": tf.shape(src_vocab.lookup(x))[0],\n",
    "                    \"tgt_length\": tf.shape(tgt_vocab.lookup(y))[0],                \n",
    "                    }, num_parallel_calls=num_threads)    \n",
    "                       \n",
    "        elif mode == \"Inference\":            \n",
    "            dataset = dataset.map(lambda x:{                    \n",
    "                    \"src_raw\": x,                \n",
    "                    \"src_ids\": src_vocab.lookup(x),                \n",
    "                    \"src_length\": tf.shape(src_vocab.lookup(x))[0],                \n",
    "                    }, num_parallel_calls=num_threads) \n",
    "            \n",
    "        elif mode == \"Predict\":            \n",
    "            dataset = dataset.map(lambda x:{\n",
    "                    \"src_raw\": x,                \n",
    "                    \"src_ids\": src_vocab.lookup(x),                \n",
    "                    \"src_length\": tf.shape(src_vocab.lookup(x))[0],                \n",
    "                    }, num_parallel_calls=num_threads)\n",
    "            \n",
    "        if mode==\"Training\":            \n",
    "            if shuffle_buffer_size is not None and shuffle_buffer_size != 0:            \n",
    "                dataset_size = get_dataset_size(src_path) \n",
    "                if dataset_size is not None:\n",
    "                    if shuffle_buffer_size < 0:\n",
    "                        shuffle_buffer_size = dataset_size\n",
    "                elif shuffle_buffer_size < dataset_size:        \n",
    "                    dataset = dataset.apply(random_shard(shuffle_buffer_size, dataset_size))        \n",
    "                dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "\n",
    "            dataset = dataset.filter(lambda x: tf.logical_and(tf.logical_and(tf.greater(x[\"src_length\"],0), tf.greater(x[\"tgt_length\"], 0)), tf.logical_and(tf.less_equal(x[\"src_length\"], max_len), tf.less_equal(x[\"tgt_length\"], max_len))))\n",
    "            \n",
    "            if bucket_width is None:\n",
    "                dataset = dataset.apply(_batch_func)\n",
    "            else:\n",
    "                if hasattr(tf.data, \"experimental\"):\n",
    "                    group_by_window_fn = tf.data.experimental.group_by_window\n",
    "                else:\n",
    "                    group_by_window_fn = tf.contrib.data.group_by_window\n",
    "                print(\"batch type: \", batch_type)\n",
    "                if batch_type == \"examples\":\n",
    "                    dataset = dataset.apply(group_by_window_fn(_key_func, _reduce_func, window_size = batch_size))\n",
    "                elif batch_type == \"tokens\":\n",
    "                    dataset = dataset.apply(group_by_window_fn(_key_func, _reduce_func, window_size_func = _window_size_func))   \n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                            \"Invalid batch type: '{}'; should be 'examples' or 'tokens'\".format(batch_type))\n",
    "            dataset = dataset.apply(filter_irregular_batches(batch_multiplier))             \n",
    "            dataset = dataset.repeat()\n",
    "            dataset = dataset.apply(prefetch_element(buffer_size=prefetch_buffer_size))                        \n",
    "        else:\n",
    "            dataset = dataset.apply(_batch_func)                      \n",
    "        \n",
    "    return dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def _compute_loss(self, outputs, tgt_ids_batch, tgt_length, params, mode):\n",
    "        \n",
    "        if mode == \"Training\":\n",
    "            mode = tf.estimator.ModeKeys.TRAIN            \n",
    "        else:\n",
    "            mode = tf.estimator.ModeKeys.EVAL            \n",
    "          \n",
    "        if self.Loss_type == \"Cross_Entropy\":\n",
    "            if isinstance(outputs, dict):\n",
    "                logits = outputs[\"logits\"]\n",
    "                attention = outputs.get(\"attention\")\n",
    "            else:\n",
    "                logits = outputs\n",
    "                attention = None \n",
    "                            \n",
    "            loss, loss_normalizer, loss_token_normalizer = cross_entropy_sequence_loss(\n",
    "                logits,\n",
    "                tgt_ids_batch, \n",
    "                tgt_length + 1,                                                         \n",
    "                label_smoothing = params.get(\"label_smoothing\", 0.0),\n",
    "                average_in_time = params.get(\"average_loss_in_time\", True),\n",
    "                mode = mode\n",
    "            )\n",
    "            return loss, loss_normalizer, loss_token_normalizer\n",
    "        \n",
    "    \n",
    "    def _initializer(self, params):\n",
    "        \n",
    "        if params[\"Architecture\"] == \"Transformer\":\n",
    "            print(\"tf.variance_scaling_initializer\")\n",
    "            return tf.variance_scaling_initializer(\n",
    "        mode=\"fan_avg\", distribution=\"uniform\", dtype=self.dtype)\n",
    "        else:            \n",
    "            param_init = params.get(\"param_init\")\n",
    "            if param_init is not None:\n",
    "                print(\"tf.random_uniform_initializer\")\n",
    "                return tf.random_uniform_initializer(\n",
    "              minval=-param_init, maxval=param_init, dtype=self.dtype)\n",
    "        return None\n",
    "        \n",
    "    def __init__(self, config_file, mode, test_feature_file=None):\n",
    "\n",
    "        def _normalize_loss(num, den=None):\n",
    "            \"\"\"Normalizes the loss.\"\"\"\n",
    "            if isinstance(num, list):  # Sharded mode.\n",
    "                if den is not None:\n",
    "                    assert isinstance(den, list)\n",
    "                    return tf.add_n(num) / tf.add_n(den) #tf.reduce_mean([num_/den_ for num_,den_ in zip(num, den)]) #tf.add_n(num) / tf.add_n(den)\n",
    "                else:\n",
    "                    return tf.reduce_mean(num)\n",
    "            elif den is not None:\n",
    "                return num / den\n",
    "            else:\n",
    "                return num\n",
    "\n",
    "        def _extract_loss(loss, Loss_type=\"Cross_Entropy\"):\n",
    "            \"\"\"Extracts and summarizes the loss.\"\"\"\n",
    "            losses = None\n",
    "            print(\"loss numb:\", len(loss))\n",
    "            if Loss_type==\"Cross_Entropy\":\n",
    "                if not isinstance(loss, tuple):                    \n",
    "                    print(1)\n",
    "                    actual_loss = _normalize_loss(loss)\n",
    "                    tboard_loss = actual_loss\n",
    "                    tf.summary.scalar(\"loss\", tboard_loss)\n",
    "                    losses = actual_loss                    \n",
    "                else:                         \n",
    "                    actual_loss = _normalize_loss(loss[0], den=loss[1])\n",
    "                    tboard_loss = _normalize_loss(loss[0], den=loss[2]) if len(loss) > 2 else actual_loss\n",
    "                    tf.summary.scalar(\"loss\", tboard_loss)            \n",
    "                    losses = actual_loss\n",
    "\n",
    "            return losses                         \n",
    "\n",
    "        def _loss_op(inputs, params, mode):\n",
    "            \"\"\"Single callable to compute the loss.\"\"\"\n",
    "            logits, _, tgt_ids_out, tgt_length  = self._build(inputs, params, mode)\n",
    "            losses = self._compute_loss(logits, tgt_ids_out, tgt_length, params, mode)\n",
    "            \n",
    "            return losses\n",
    "\n",
    "        with open(config_file, \"r\") as stream:\n",
    "            config = yaml.load(stream)\n",
    "\n",
    "        Loss_type = config.get(\"Loss_Function\",\"Cross_Entropy\")\n",
    "        \n",
    "        self.Loss_type = Loss_type\n",
    "        self.config = config \n",
    "        self.using_tf_idf = config.get(\"using_tf_idf\", False)\n",
    "        \n",
    "        train_batch_size = config[\"training_batch_size\"]   \n",
    "        eval_batch_size = config[\"eval_batch_size\"]\n",
    "        max_len = config[\"max_len\"]\n",
    "        \n",
    "        example_sampling_distribution = config.get(\"example_sampling_distribution\",None)\n",
    "        self.dtype = tf.float32\n",
    "        \n",
    "        # Input pipeline:\n",
    "        # Return lookup table of type index_table_from_file\n",
    "        src_vocab, _ = load_vocab(config[\"src_vocab_path\"], config[\"src_vocab_size\"])\n",
    "        tgt_vocab, _ = load_vocab(config[\"tgt_vocab_path\"], config[\"tgt_vocab_size\"])\n",
    "        \n",
    "        load_data_version = config.get(\"dataprocess_version\",None)\n",
    "        \n",
    "        if mode == \"Training\":    \n",
    "            print(\"num_devices\", config.get(\"num_devices\",1))\n",
    "            \n",
    "            dispatcher = GraphDispatcher(\n",
    "                config.get(\"num_devices\",1), \n",
    "                daisy_chain_variables=config.get(\"daisy_chain_variables\",False), \n",
    "                devices= config.get(\"devices\",None)\n",
    "            ) \n",
    "            \n",
    "            batch_multiplier = config.get(\"num_devices\", 1)\n",
    "            num_threads = config.get(\"num_threads\", 4)\n",
    "            \n",
    "            if Loss_type == \"Wasserstein\":\n",
    "                self.using_tf_idf = True\n",
    "                \n",
    "            if self.using_tf_idf:\n",
    "                tf_idf_table = build_tf_idf_table(\n",
    "                    config[\"tgt_vocab_path\"], \n",
    "                    config[\"tgt_vocab_size\"], \n",
    "                    config[\"domain_numb\"], \n",
    "                    config[\"training_feature_file\"])           \n",
    "                self.tf_idf_table = tf_idf_table\n",
    "                \n",
    "            iterator = load_data(\n",
    "                config[\"training_label_file\"], \n",
    "                src_vocab, \n",
    "                batch_size = train_batch_size, \n",
    "                batch_type=config[\"training_batch_type\"], \n",
    "                batch_multiplier = batch_multiplier, \n",
    "                tgt_path=config[\"training_feature_file\"], \n",
    "                tgt_vocab=tgt_vocab, \n",
    "                max_len = max_len, \n",
    "                mode=mode, \n",
    "                shuffle_buffer_size = config[\"sample_buffer_size\"], \n",
    "                num_threads = num_threads, \n",
    "                version = load_data_version, \n",
    "                distribution = example_sampling_distribution\n",
    "            )\n",
    "            \n",
    "            inputs = iterator.get_next()\n",
    "            data_shards = dispatcher.shard(inputs)\n",
    "\n",
    "            with tf.variable_scope(config[\"Architecture\"], initializer=self._initializer(config)):\n",
    "                losses_shards = dispatcher(_loss_op, data_shards, config, mode)\n",
    "\n",
    "            self.loss = _extract_loss(losses_shards, Loss_type=Loss_type) \n",
    "\n",
    "        elif mode == \"Inference\": \n",
    "            assert test_feature_file != None\n",
    "            \n",
    "            iterator = load_data(\n",
    "                test_feature_file, \n",
    "                src_vocab, \n",
    "                batch_size = eval_batch_size, \n",
    "                batch_type = \"examples\", \n",
    "                batch_multiplier = 1, \n",
    "                max_len = max_len, \n",
    "                mode = mode, \n",
    "                version = load_data_version\n",
    "            )\n",
    "            \n",
    "            inputs = iterator.get_next() \n",
    "            \n",
    "            with tf.variable_scope(config[\"Architecture\"]):\n",
    "                _ , self.predictions, _, _ = self._build(inputs, config, mode)\n",
    "            \n",
    "        self.iterator = iterator\n",
    "        self.inputs = inputs\n",
    "        \n",
    "    def loss_(self):\n",
    "        return self.loss\n",
    "    \n",
    "    def prediction_(self):\n",
    "        return self.predictions\n",
    "   \n",
    "    def inputs_(self):\n",
    "        return self.inputs\n",
    "    \n",
    "    def iterator_initializers(self):\n",
    "        if isinstance(self.iterator,list):\n",
    "            return [iterator.initializer for iterator in self.iterator]\n",
    "        else:\n",
    "            return [self.iterator.initializer]        \n",
    "           \n",
    "    def _build(self, inputs, config, mode):        \n",
    "\n",
    "        debugging = config.get(\"debugging\", False)\n",
    "        Loss_type = self.Loss_type       \n",
    "        print(\"Loss_type: \", Loss_type)           \n",
    "\n",
    "        hidden_size = config[\"hidden_size\"]       \n",
    "        print(\"hidden size: \", hidden_size)\n",
    "                \n",
    "        tgt_vocab_rev = tf.contrib.lookup.index_to_string_table_from_file(config[\"tgt_vocab_path\"], vocab_size= int(config[\"tgt_vocab_size\"]) - 1, default_value=constants.UNKNOWN_TOKEN)\n",
    "        end_token = constants.END_OF_SENTENCE_ID\n",
    "        # Embedding        \n",
    "        size_src = config.get(\"src_embedding_size\",512)\n",
    "        size_tgt = config.get(\"tgt_embedding_size\",512)\n",
    "        with tf.variable_scope(\"src_embedding\"):\n",
    "            src_emb = create_embeddings(config[\"src_vocab_size\"], depth=size_src)\n",
    "\n",
    "        with tf.variable_scope(\"tgt_embedding\"):\n",
    "            tgt_emb = create_embeddings(config[\"tgt_vocab_size\"], depth=size_tgt)\n",
    "\n",
    "        self.tgt_emb = tgt_emb\n",
    "        self.src_emb = src_emb\n",
    "\n",
    "        # Build encoder, decoder\n",
    "        if config[\"Architecture\"] == \"GRU\":\n",
    "            nlayers = config.get(\"nlayers\",4)\n",
    "            encoder = onmt.encoders.BidirectionalRNNEncoder(nlayers, hidden_size, reducer=onmt.layers.ConcatReducer(), cell_class = tf.contrib.rnn.GRUCell, dropout=0.1, residual_connections=True)\n",
    "            decoder = onmt.decoders.AttentionalRNNDecoder(nlayers, hidden_size, bridge=onmt.layers.CopyBridge(), cell_class=tf.contrib.rnn.GRUCell, dropout=0.1, residual_connections=True)\n",
    "        elif config[\"Architecture\"] == \"LSTM\":\n",
    "            nlayers = config.get(\"nlayers\",4)\n",
    "            encoder = onmt.encoders.BidirectionalRNNEncoder(nlayers, num_units=hidden_size, reducer=onmt.layers.ConcatReducer(), cell_class=tf.nn.rnn_cell.LSTMCell,\n",
    "                                                          dropout=0.1, residual_connections=True)\n",
    "            decoder = onmt.decoders.AttentionalRNNDecoder(nlayers, num_units=hidden_size, bridge=onmt.layers.CopyBridge(), attention_mechanism_class=tf.contrib.seq2seq.LuongAttention,\n",
    "                                                         cell_class=tf.nn.rnn_cell.LSTMCell, dropout=0.1, residual_connections=True)\n",
    "        elif config[\"Architecture\"] == \"Transformer\":\n",
    "            nlayers = config.get(\"nlayers\",6)\n",
    "            decoder = onmt.decoders.self_attention_decoder.SelfAttentionDecoder(nlayers, num_units=hidden_size, num_heads=8, ffn_inner_dim=2048, dropout=0.1, attention_dropout=0.1, relu_dropout=0.1)\n",
    "            encoder = onmt.encoders.self_attention_encoder.SelfAttentionEncoder(nlayers, num_units=hidden_size, num_heads=8, ffn_inner_dim=2048, dropout=0.1, attention_dropout=0.1, relu_dropout=0.1)       \n",
    "        print(\"Model type: \", config[\"Architecture\"])\n",
    "\n",
    "        if mode ==\"Training\":            \n",
    "            print(\"Building model in Training mode\")\n",
    "        elif mode == \"Inference\":\n",
    "            print(\"Build model in Inference mode\")\n",
    "        start_tokens = tf.fill([tf.shape(inputs[\"src_ids\"])[0]], constants.START_OF_SENTENCE_ID)\n",
    "                    \n",
    "        emb_src_batch = tf.nn.embedding_lookup(src_emb, inputs[\"src_ids\"]) # dim = [batch, length, depth]\n",
    "\n",
    "        self.emb_src_batch = emb_src_batch\n",
    "        print(\"emb_src_batch: \", emb_src_batch)\n",
    " \n",
    "        if mode==\"Training\":\n",
    "            emb_tgt_batch = tf.nn.embedding_lookup(tgt_emb, inputs[\"tgt_ids_in\"])    \n",
    "            self.emb_tgt_batch = emb_tgt_batch\n",
    "            print(\"emb_tgt_batch: \", emb_tgt_batch)                   \n",
    "                \n",
    "        src_length = inputs[\"src_length\"]\n",
    "        \n",
    "        if mode ==\"Training\":\n",
    "            tgt_ids_batch = inputs[\"tgt_ids_out\"]\n",
    "            \n",
    "        with tf.variable_scope(\"encoder\", reuse=tf.AUTO_REUSE):\n",
    "            if mode==\"Training\":\n",
    "                encoder_output = encoder.encode(emb_src_batch, sequence_length = src_length, mode=tf.estimator.ModeKeys.TRAIN)\n",
    "            else:\n",
    "                encoder_output = encoder.encode(emb_src_batch, sequence_length = src_length, mode=tf.estimator.ModeKeys.PREDICT)\n",
    "            self.encoder_output = encoder_output\n",
    "        tgt_length = inputs[\"tgt_length\"]\n",
    "        output_layer = None\n",
    "        if mode == \"Training\":    \n",
    "            if Loss_type == \"Cross_Entropy\":\n",
    "                with tf.variable_scope(\"decoder\"):                           \n",
    "                    if config.get(\"Standard\",True):\n",
    "                        logits, _, _, attention = decoder.decode(\n",
    "                                              emb_tgt_batch, \n",
    "                                              tgt_length + 1,\n",
    "                                              vocab_size = int(config[\"tgt_vocab_size\"]),\n",
    "                                              initial_state = encoder_output[1],\n",
    "                                              output_layer = output_layer,                                              \n",
    "                                              mode = tf.estimator.ModeKeys.TRAIN,\n",
    "                                              memory = encoder_output[0],\n",
    "                                              memory_sequence_length = encoder_output[2],\n",
    "                                              return_alignment_history = True) \n",
    "                    else:\n",
    "                        logits, _, _, attention = decoder.decode(\n",
    "                                              emb_tgt_batch,\n",
    "                                              tgt_length + 1,\n",
    "                                              emb_mask = position_mask_tgt,\n",
    "                                              vocab_size = int(config[\"tgt_vocab_size\"]),\n",
    "                                              initial_state = encoder_output[1],\n",
    "                                              output_layer = output_layer,\n",
    "                                              mode = tf.estimator.ModeKeys.TRAIN,\n",
    "                                              memory = encoder_output[0],\n",
    "                                              memory_sequence_length = encoder_output[2],\n",
    "                                              return_alignment_history = True)\n",
    "                    outputs = {\n",
    "                           \"logits\": logits\n",
    "                           }           \n",
    "\n",
    "        if mode != \"Training\":\n",
    "                            \n",
    "            with tf.variable_scope(\"decoder\"):        \n",
    "                beam_width = config.get(\"beam_width\", 5)\n",
    "                print(\"Inference with beam width %d\"%(beam_width))\n",
    "                maximum_iterations = config.get(\"maximum_iterations\", 250)\n",
    "               \n",
    "                if beam_width <= 1:                \n",
    "                    sampled_ids, _, sampled_length, log_probs, alignment = decoder.dynamic_decode(\n",
    "                                                                                    tgt_emb,\n",
    "                                                                                    start_tokens,\n",
    "                                                                                    end_token,\n",
    "                                                                                    vocab_size=int(config[\"tgt_vocab_size\"]),\n",
    "                                                                                    initial_state=encoder_output[1],\n",
    "                                                                                    maximum_iterations=maximum_iterations,\n",
    "                                                                                    output_layer = output_layer,\n",
    "                                                                                    mode=tf.estimator.ModeKeys.PREDICT,\n",
    "                                                                                    memory=encoder_output[0],\n",
    "                                                                                    memory_sequence_length=encoder_output[2],\n",
    "                                                                                    dtype=tf.float32,\n",
    "                                                                                    return_alignment_history=True)\n",
    "                else:\n",
    "                    length_penalty = config.get(\"length_penalty\", 0)\n",
    "                    sampled_ids, _, sampled_length, log_probs, alignment = decoder.dynamic_decode_and_search(\n",
    "                                                          tgt_emb,\n",
    "                                                          start_tokens,\n",
    "                                                          end_token,\n",
    "                                                          vocab_size = int(config[\"tgt_vocab_size\"]),\n",
    "                                                          initial_state = encoder_output[1],\n",
    "                                                          beam_width = beam_width,\n",
    "                                                          length_penalty = length_penalty,\n",
    "                                                          maximum_iterations = maximum_iterations,\n",
    "                                                          output_layer = output_layer,\n",
    "                                                          mode = tf.estimator.ModeKeys.PREDICT,\n",
    "                                                          memory = encoder_output[0],\n",
    "                                                          memory_sequence_length = encoder_output[2],\n",
    "                                                          dtype=tf.float32,\n",
    "                                                          return_alignment_history = True)\n",
    "                    \n",
    "                   \n",
    "            target_tokens = tgt_vocab_rev.lookup(tf.cast(sampled_ids, tf.int64))\n",
    "            \n",
    "            predictions = {\n",
    "              \"tokens\": target_tokens,\n",
    "              \"length\": sampled_length,\n",
    "              \"log_probs\": log_probs,\n",
    "              \"alignment\": alignment,\n",
    "            }\n",
    "            tgt_ids_batch = None\n",
    "            tgt_length = None\n",
    "        else:\n",
    "            predictions = None\n",
    "\n",
    "        self.outputs = outputs\n",
    "        \n",
    "        return outputs, predictions, tgt_ids_batch, tgt_length         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import opennmt as onmt\n",
    "from opennmt.utils.optim import *\n",
    "from utils.dataprocess import *\n",
    "from utils.utils_ import *\n",
    "import argparse\n",
    "import sys\n",
    "import numpy as np\n",
    "from opennmt.inputters.text_inputter import load_pretrained_embeddings\n",
    "from opennmt.utils.losses import cross_entropy_sequence_loss\n",
    "from opennmt.utils.evaluator import *\n",
    "from model import *\n",
    "\n",
    "import ipdb\n",
    "\n",
    "import io\n",
    "from tensorflow.python.framework import ops\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'config_tuanh.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tnguyen/anaconda3/envs/py3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "with open(config_file, \"r\") as stream:\n",
    "    config = yaml.load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['optimizer_parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['model_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval directory stores prediction files\n",
    "if not os.path.exists(os.path.join(config[\"model_dir\"],\"eval\")):\n",
    "    os.makedirs(os.path.join(config[\"model_dir\"],\"eval\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tnguyen/anaconda3/envs/py3.6/lib/python3.6/site-packages/ipykernel_launcher.py:85: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_devices 2\n",
      "batch_size 6144\n",
      "old dataprocessing version\n",
      "batch type:  tokens\n",
      "tf.variance_scaling_initializer\n",
      "Loss_type:  Cross_Entropy\n",
      "hidden size:  512\n",
      "Model type:  Transformer\n",
      "Building model in Training mode\n",
      "emb_src_batch:  Tensor(\"Transformer/parallel_0/Transformer/embedding_lookup:0\", shape=(?, ?, 512), dtype=float32, device=/device:GPU:0)\n",
      "emb_tgt_batch:  Tensor(\"Transformer/parallel_0/Transformer/embedding_lookup_1:0\", shape=(?, ?, 512), dtype=float32, device=/device:GPU:0)\n",
      "Loss_type:  Cross_Entropy\n",
      "hidden size:  512\n",
      "Model type:  Transformer\n",
      "Building model in Training mode\n",
      "emb_src_batch:  Tensor(\"Transformer/parallel_1/Transformer/embedding_lookup:0\", shape=(?, ?, 512), dtype=float32, device=/device:GPU:1)\n",
      "emb_tgt_batch:  Tensor(\"Transformer/parallel_1/Transformer/embedding_lookup_1:0\", shape=(?, ?, 512), dtype=float32, device=/device:GPU:1)\n",
      "loss numb: 3\n"
     ]
    }
   ],
   "source": [
    "training_model = Model(config_file, \"Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.train.create_global_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.get(\"Loss_Function\",\"Cross_Entropy\")==\"Cross_Entropy\":\n",
    "     generator_total_loss = training_model.loss_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = training_model.inputs_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['table_initializer', 'asset_filepaths', 'variables', 'trainable_variables', 'update_ops', 'model_variables', 'summaries', 'global_step']\n"
     ]
    }
   ],
   "source": [
    "print(tf.get_default_graph().get_all_collection_keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.framework.ops.Graph object at 0x7fd4774e2080>\n"
     ]
    }
   ],
   "source": [
    "print(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"mode\"] == \"Training\":\n",
    "    optimizer_params = config[\"optimizer_parameters\"]\n",
    "    with tf.variable_scope(\"main_training\"):\n",
    "        train_op, accum_vars_ = optimize_loss(generator_total_loss, config[\"optimizer_parameters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation set:  1\n"
     ]
    }
   ],
   "source": [
    "Eval_dataset_numb = len(config[\"eval_label_file\"])\n",
    "print(\"Number of validation set: \", Eval_dataset_numb)\n",
    "external_evaluator = [None] * Eval_dataset_numb \n",
    "writer_bleu = [None] * Eval_dataset_numb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(Eval_dataset_numb):\n",
    "    external_evaluator[i] = BLEUEvaluator(config[\"eval_label_file\"][i], config[\"model_dir\"])\n",
    "    writer_bleu[i] = tf.summary.FileWriter(os.path.join(config[\"model_dir\"],\"BLEU\",\"domain_%d\"%i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"eval_label_file\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(config=tf.ConfigProto(log_device_placement=False, allow_soft_placement=True, gpu_options=tf.GPUOptions(allow_growth=True))) as sess:\n",
    "    \n",
    "    writer = tf.summary.FileWriter(config[\"model_dir\"])    \n",
    "    var_list_ = tf.global_variables()\n",
    "    for v in tf.trainable_variables():\n",
    "        if v not in tf.global_variables():\n",
    "            var_list_.append(v)\n",
    "    for v in var_list_:\n",
    "        print(v.name)\n",
    "    saver = tf.train.Saver(var_list_, max_to_keep=config[\"max_to_keep\"])\n",
    "    checkpoint_path = tf.train.latest_checkpoint(config[\"model_dir\"])\n",
    "        \n",
    "    sess.run([v.initializer for v in var_list_])    \n",
    "\n",
    "    sess.run([v.initializer for v in accum_vars_])\n",
    "    \n",
    "    training_summary = tf.summary.merge_all()\n",
    "    global_step_ = sess.run(global_step) \n",
    "    if checkpoint_path:\n",
    "        print(\"Continue training:...\")\n",
    "        print(\"Load parameters from %s\"%checkpoint_path)\n",
    "        saver.restore(sess, checkpoint_path)        \n",
    "        global_step_ = sess.run(global_step)\n",
    "        print(\"global_step: \", global_step_)\n",
    "                            \n",
    "        for i in range(Eval_dataset_numb):\n",
    "            prediction_file = inference(config_file, checkpoint_path, config[\"eval_feature_file\"][i])\n",
    "            score = external_evaluator[i].score(config[\"eval_label_file\"][i], prediction_file)\n",
    "            print(\"BLEU at checkpoint %s for testset %s: %f\"%(checkpoint_path, config[\"eval_feature_file\"][i], score))            \n",
    "                \n",
    "    else:\n",
    "        print(\"Training from scratch\")\n",
    "        \n",
    "    tf.tables_initializer().run()    \n",
    "    sess.run(training_model.iterator_initializers())\n",
    "    total_loss = []            \n",
    "   \n",
    "    while global_step_ <= config[\"iteration_number\"]:                       \n",
    "\n",
    "        loss_, global_step_, _ = sess.run([generator_total_loss, global_step, train_op])               \n",
    "        total_loss.append(loss_)\n",
    "        \n",
    "        if (np.mod(global_step_, config[\"printing_freq\"])) == 0:            \n",
    "            print((datetime.datetime.now()))\n",
    "            print((\"Loss at step %d\"%(global_step_), np.mean(total_loss)))                \n",
    "            \n",
    "        if (np.mod(global_step_, config[\"summary_freq\"])) == 0:\n",
    "            training_summary_ = sess.run(training_summary)\n",
    "            writer.add_summary(training_summary_, global_step=global_step_)\n",
    "            writer.flush()\n",
    "            total_loss = []\n",
    "            \n",
    "        if (np.mod(global_step_, config[\"save_freq\"])) == 0 and global_step_ > 0:    \n",
    "            print((datetime.datetime.now()))\n",
    "            checkpoint_path = os.path.join(config[\"model_dir\"], 'model.ckpt')\n",
    "            print((\"save to %s\"%(checkpoint_path)))\n",
    "            saver.save(sess, checkpoint_path, global_step = global_step_)\n",
    "                                                                                                                 \n",
    "        if (np.mod(global_step_, config[\"eval_freq\"])) == 0 and global_step_ >0: \n",
    "            checkpoint_path = tf.train.latest_checkpoint(config[\"model_dir\"])\n",
    "            for i in range(Eval_dataset_numb):\n",
    "                prediction_file = inference(config_file, checkpoint_path, config[\"eval_feature_file\"][i])\n",
    "                score = external_evaluator[i].score(config[\"eval_label_file\"][i], prediction_file)\n",
    "                print(\"BLEU at checkpoint %s for testset %s: %f\"%(checkpoint_path,config[\"eval_label_file\"][i], score))\n",
    "                score_summary = tf.Summary(value=[tf.Summary.Value(tag=\"eval_score_%d\"%i, simple_value=score)])\n",
    "                writer_bleu[i].add_summary(score_summary, global_step_)\n",
    "                writer_bleu[i].flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3.6]",
   "language": "python",
   "name": "conda-env-py3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
