{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import opennmt as onmt\n",
    "from opennmt.utils.misc import count_lines\n",
    "from opennmt.utils.parallel import GraphDispatcher\n",
    "from opennmt import constants\n",
    "from opennmt.utils.losses import cross_entropy_sequence_loss\n",
    "from opennmt.utils.optim import *\n",
    "from opennmt.utils.evaluator import *\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import numpy as np\n",
    "import datetime\n",
    "from opennmt.utils.cell import build_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(vocab_path, vocab_size):\n",
    "    if not vocab_size:\n",
    "        vocab_size = count_lines(vocab_path) + 1 #for UNK\n",
    "        print(\"vocab size of\",vocab_path,\":\",vocab_size)\n",
    "    vocab = tf.contrib.lookup.index_table_from_file(vocab_path, vocab_size = vocab_size - 1, num_oov_buckets = 1)\n",
    "    return vocab, vocab_size\n",
    "\n",
    "def get_dataset_size(data_file):\n",
    "    return count_lines(data_file)\n",
    "\n",
    "def get_padded_shapes(dataset):    \n",
    "    return tf.contrib.framework.nest.map_structure(\n",
    "    lambda shape: shape.as_list(), dataset.output_shapes)\n",
    "\n",
    "def filter_irregular_batches(multiple):    \n",
    "    if multiple == 1:\n",
    "        return lambda dataset: dataset\n",
    "\n",
    "    def _predicate(*x):\n",
    "        flat = tf.contrib.framework.nest.flatten(x)\n",
    "        batch_size = tf.shape(flat[0])[0]\n",
    "        return tf.equal(tf.mod(batch_size, multiple), 0)\n",
    "\n",
    "    return lambda dataset: dataset.filter(_predicate)\n",
    "\n",
    "def prefetch_element(buffer_size=None):  \n",
    "    support_auto_tuning = hasattr(tf.data, \"experimental\") or hasattr(tf.contrib.data, \"AUTOTUNE\")\n",
    "    if not support_auto_tuning and buffer_size is None:\n",
    "        buffer_size = 1\n",
    "    return lambda dataset: dataset.prefetch(buffer_size)\n",
    "\n",
    "def create_embeddings(vocab_size, depth=512):\n",
    "      \"\"\"Creates an embedding variable.\"\"\"\n",
    "      return tf.get_variable(\"embedding\", shape = [vocab_size, depth])\n",
    "\n",
    "def kl_coeff(i):\n",
    "    # coeff = (tf.tanh((i - 3500)/1000) + 1)/2\n",
    "    coeff = (tf.tanh((i - 20000)/5000) + 1)/2\n",
    "    return tf.cast(coeff, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(src_path, src_vocab, batch_size=32, batch_type =\"examples\", batch_multiplier = 1, tgt_path=None, tgt_vocab=None, \n",
    "              max_len=50, bucket_width = 1, mode=\"Training\", padded_shapes = None, \n",
    "              shuffle_buffer_size = None, prefetch_buffer_size = 100000, num_threads = 4, version=None, distribution=None, tf_idf_table=None):\n",
    "\n",
    "    batch_size = batch_size * batch_multiplier\n",
    "    print(\"batch_size\", batch_size)\n",
    "    \n",
    "    def _make_dataset(text_path):\n",
    "        dataset = tf.data.TextLineDataset(text_path)\n",
    "        dataset = dataset.map(lambda x: tf.string_split([x]).values) #split by spaces\n",
    "        return dataset    \n",
    "       \n",
    "    def _batch_func(dataset):\n",
    "        return dataset.padded_batch(batch_size,\n",
    "                                    padded_shapes=padded_shapes or get_padded_shapes(dataset))\n",
    "\n",
    "    def _key_func(dataset):                \n",
    "        #bucket_id = tf.squeeze(dataset[\"domain\"])\n",
    "        features_length = dataset[\"src_length\"] #features_length_fn(features) if features_length_fn is not None else None\n",
    "        labels_length = dataset[\"tgt_length\"] #labels_length_fn(labels) if labels_length_fn is not None else None        \n",
    "        bucket_id = tf.constant(0, dtype=tf.int32)\n",
    "        if features_length is not None:\n",
    "            bucket_id = tf.maximum(bucket_id, features_length // bucket_width)\n",
    "        if labels_length is not None:\n",
    "            bucket_id = tf.maximum(bucket_id, labels_length // bucket_width)\n",
    "        return tf.cast(bucket_id, tf.int64)\n",
    "        #return tf.to_int64(bucket_id)\n",
    "\n",
    "    def _reduce_func(unused_key, dataset):\n",
    "        return _batch_func(dataset)\n",
    "\n",
    "    def _window_size_func(key):\n",
    "        if bucket_width > 1:\n",
    "            key += 1  # For bucket_width == 1, key 0 is unassigned.\n",
    "        size = batch_size // (key * bucket_width)\n",
    "        if batch_multiplier > 1:\n",
    "            # Make the window size a multiple of batch_multiplier.\n",
    "            size = size + batch_multiplier - size % batch_multiplier\n",
    "        return tf.to_int64(tf.maximum(size, batch_multiplier))             \n",
    "    \n",
    "    bos = tf.constant([constants.START_OF_SENTENCE_ID], dtype=tf.int64)\n",
    "    eos = tf.constant([constants.END_OF_SENTENCE_ID], dtype=tf.int64)\n",
    "    \n",
    "    if version==None:\n",
    "        print(\"old dataprocessing version\")\n",
    "        src_dataset = _make_dataset(src_path)            \n",
    "        if mode==\"Training\":\n",
    "            tgt_dataset = _make_dataset(tgt_path)\n",
    "            dataset = tf.data.Dataset.zip((src_dataset, tgt_dataset))\n",
    "        elif mode==\"Inference\":\n",
    "            dataset = src_dataset\n",
    "        elif mode == \"Predict\":\n",
    "            dataset = src_dataset\n",
    "\n",
    "        if mode==\"Training\":                    \n",
    "            dataset = dataset.map(lambda x,y:{                      \n",
    "                    \"src_raw\": x,\n",
    "                    \"tgt_raw\": y,\n",
    "                    \"src_ids\": src_vocab.lookup(x),\n",
    "                    \"tgt_ids\": tgt_vocab.lookup(y),\n",
    "                    \"tgt_ids_in\": tf.concat([bos, tgt_vocab.lookup(y)], axis=0),\n",
    "                    \"tgt_ids_out\": tf.concat([tgt_vocab.lookup(y), eos], axis=0),\n",
    "                    \"src_length\": tf.shape(src_vocab.lookup(x))[0],\n",
    "                    \"tgt_length\": tf.shape(tgt_vocab.lookup(y))[0],                \n",
    "                    }, num_parallel_calls=num_threads)    \n",
    "                       \n",
    "        elif mode == \"Inference\":            \n",
    "            dataset = dataset.map(lambda x:{                    \n",
    "                    \"src_raw\": x,                \n",
    "                    \"src_ids\": src_vocab.lookup(x),                \n",
    "                    \"src_length\": tf.shape(src_vocab.lookup(x))[0],                \n",
    "                    }, num_parallel_calls=num_threads) \n",
    "            \n",
    "        elif mode == \"Predict\":            \n",
    "            dataset = dataset.map(lambda x:{\n",
    "                    \"src_raw\": x,                \n",
    "                    \"src_ids\": src_vocab.lookup(x),                \n",
    "                    \"src_length\": tf.shape(src_vocab.lookup(x))[0],                \n",
    "                    }, num_parallel_calls=num_threads)\n",
    "            \n",
    "        if mode==\"Training\":            \n",
    "            if shuffle_buffer_size is not None and shuffle_buffer_size != 0:            \n",
    "                dataset_size = get_dataset_size(src_path) \n",
    "                if dataset_size is not None:\n",
    "                    if shuffle_buffer_size < 0:\n",
    "                        shuffle_buffer_size = dataset_size\n",
    "                elif shuffle_buffer_size < dataset_size:        \n",
    "                    dataset = dataset.apply(random_shard(shuffle_buffer_size, dataset_size))        \n",
    "                dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "\n",
    "            dataset = dataset.filter(lambda x: tf.logical_and(tf.logical_and(tf.greater(x[\"src_length\"],0), tf.greater(x[\"tgt_length\"], 0)), tf.logical_and(tf.less_equal(x[\"src_length\"], max_len), tf.less_equal(x[\"tgt_length\"], max_len))))\n",
    "            \n",
    "            if bucket_width is None:\n",
    "                dataset = dataset.apply(_batch_func)\n",
    "            else:\n",
    "                if hasattr(tf.data, \"experimental\"):\n",
    "                    group_by_window_fn = tf.data.experimental.group_by_window\n",
    "                else:\n",
    "                    group_by_window_fn = tf.contrib.data.group_by_window\n",
    "                print(\"batch type: \", batch_type)\n",
    "                if batch_type == \"examples\":\n",
    "                    dataset = dataset.apply(group_by_window_fn(_key_func, _reduce_func, window_size = batch_size))\n",
    "                elif batch_type == \"tokens\":\n",
    "                    dataset = dataset.apply(group_by_window_fn(_key_func, _reduce_func, window_size_func = _window_size_func))   \n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                            \"Invalid batch type: '{}'; should be 'examples' or 'tokens'\".format(batch_type))\n",
    "            dataset = dataset.apply(filter_irregular_batches(batch_multiplier))             \n",
    "            dataset = dataset.repeat()\n",
    "            dataset = dataset.apply(prefetch_element(buffer_size=prefetch_buffer_size))                        \n",
    "        else:\n",
    "            dataset = dataset.apply(_batch_func)                      \n",
    "        \n",
    "    return dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def _compute_loss(self, outputs, tgt_ids_batch, tgt_length, params, mode, mu_states, logvar_states):\n",
    "        \n",
    "        if mode == \"Training\":\n",
    "            mode = tf.estimator.ModeKeys.TRAIN            \n",
    "        else:\n",
    "            mode = tf.estimator.ModeKeys.EVAL            \n",
    "          \n",
    "        if self.Loss_type == \"Cross_Entropy\":\n",
    "            if isinstance(outputs, dict):\n",
    "                logits = outputs[\"logits\"]\n",
    "                attention = outputs.get(\"attention\")\n",
    "            else:\n",
    "                logits = outputs\n",
    "                attention = None \n",
    "                            \n",
    "            loss, loss_normalizer, loss_token_normalizer = cross_entropy_sequence_loss(\n",
    "                logits,\n",
    "                tgt_ids_batch, \n",
    "                tgt_length + 1,                                                         \n",
    "                label_smoothing = params.get(\"label_smoothing\", 0.0),\n",
    "                average_in_time = params.get(\"average_loss_in_time\", True),\n",
    "                mode = mode\n",
    "            )\n",
    "            \n",
    "            \n",
    "            #----- Calculating kl divergence --------\n",
    "\n",
    "            kld_loss = -0.5 * tf.reduce_sum(logvar_states - self.logvar_0 - tf.pow(mu_states-self.mu_0, 2)/tf.exp(self.logvar_0) - tf.exp(logvar_states)/tf.exp(self.logvar_0) + 1, 1)\n",
    "\n",
    "            return loss, loss_normalizer, loss_token_normalizer, kld_loss\n",
    "        \n",
    "    \n",
    "    def _initializer(self, params):\n",
    "        \n",
    "        if params[\"Architecture\"] == \"Transformer\":\n",
    "            print(\"tf.variance_scaling_initializer\")\n",
    "            return tf.variance_scaling_initializer(\n",
    "        mode=\"fan_avg\", distribution=\"uniform\", dtype=self.dtype)\n",
    "        else:            \n",
    "            param_init = params.get(\"param_init\")\n",
    "            if param_init is not None:\n",
    "                print(\"tf.random_uniform_initializer\")\n",
    "                return tf.random_uniform_initializer(\n",
    "              minval=-param_init, maxval=param_init, dtype=self.dtype)\n",
    "        return None\n",
    "        \n",
    "    def __init__(self, config_file, mode, test_feature_file=None):\n",
    "\n",
    "        def _normalize_loss(num, den=None):\n",
    "            \"\"\"Normalizes the loss.\"\"\"\n",
    "            if isinstance(num, list):  # Sharded mode.\n",
    "                if den is not None:\n",
    "                    assert isinstance(den, list)\n",
    "                    return tf.add_n(num) / tf.add_n(den) #tf.reduce_mean([num_/den_ for num_,den_ in zip(num, den)]) #tf.add_n(num) / tf.add_n(den)\n",
    "                else:\n",
    "                    return tf.reduce_mean(num)\n",
    "            elif den is not None:\n",
    "                return num / den\n",
    "            else:\n",
    "                return num\n",
    "\n",
    "        def _extract_loss(loss, Loss_type=\"Cross_Entropy\"):\n",
    "            \"\"\"Extracts and summarizes the loss.\"\"\"\n",
    "            losses = None\n",
    "            print(\"loss numb:\", len(loss))\n",
    "            if Loss_type==\"Cross_Entropy\":\n",
    "                if not isinstance(loss, tuple):                    \n",
    "                    print(1)\n",
    "                    actual_loss = _normalize_loss(loss)\n",
    "                    tboard_loss = actual_loss\n",
    "                    tf.summary.scalar(\"loss\", tboard_loss)\n",
    "                    losses = actual_loss                    \n",
    "                else:                         \n",
    "                    actual_loss = _normalize_loss(loss[0], den=loss[1])\n",
    "                    tboard_loss = _normalize_loss(loss[0], den=loss[2]) if len(loss) > 2 else actual_loss\n",
    "                    losses = actual_loss\n",
    "                    loss_kd = _normalize_loss(loss[3])\n",
    "                    tf.summary.scalar(\"loss\", tboard_loss)\n",
    "                    tf.summary.scalar(\"kl_loss\", loss_kd)\n",
    "\n",
    "            return losses,loss_kd                         \n",
    "\n",
    "        def _loss_op(inputs, params, mode):\n",
    "            \"\"\"Single callable to compute the loss.\"\"\"\n",
    "            logits, _, tgt_ids_out, tgt_length, mu_states, logvar_states  = self._build(inputs, params, mode)\n",
    "            losses = self._compute_loss(logits, tgt_ids_out, tgt_length, params, mode, mu_states, logvar_states)\n",
    "            \n",
    "            return losses\n",
    "\n",
    "        with open(config_file, \"r\") as stream:\n",
    "            config = yaml.load(stream)\n",
    "        \n",
    "        Loss_type = config.get(\"Loss_Function\",\"Cross_Entropy\")\n",
    "        \n",
    "        self.Loss_type = Loss_type\n",
    "        self.config = config \n",
    "        self.using_tf_idf = config.get(\"using_tf_idf\", False)\n",
    "        \n",
    "        train_batch_size = config[\"training_batch_size\"]   \n",
    "        eval_batch_size = config[\"eval_batch_size\"]\n",
    "        \n",
    "        self.latent_variable_size = config.get(\"latent_variable_size\",128)\n",
    "        \n",
    "        max_len = config[\"max_len\"]\n",
    "        \n",
    "        example_sampling_distribution = config.get(\"example_sampling_distribution\",None)\n",
    "        self.dtype = tf.float32\n",
    "        \n",
    "        # Input pipeline:\n",
    "        # Return lookup table of type index_table_from_file\n",
    "        src_vocab, src_vocab_size = load_vocab(config[\"src_vocab_path\"], config.get(\"src_vocab_size\", None))\n",
    "        tgt_vocab, tgt_vocab_size = load_vocab(config[\"tgt_vocab_path\"], config.get(\"tgt_vocab_size\", None))\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        \n",
    "        # define mu0_ and logvar0_\n",
    "        self.mu_0 = .0\n",
    "        self.logvar_0 = .0\n",
    "        \n",
    "        load_data_version = config.get(\"dataprocess_version\",None)\n",
    "        \n",
    "        if mode == \"Training\":    \n",
    "            print(\"num_devices\", config.get(\"num_devices\",1))\n",
    "            \n",
    "            dispatcher = GraphDispatcher(\n",
    "                config.get(\"num_devices\",1), \n",
    "                daisy_chain_variables=config.get(\"daisy_chain_variables\",False), \n",
    "                devices= config.get(\"devices\",None)\n",
    "            ) \n",
    "            \n",
    "            batch_multiplier = config.get(\"num_devices\", 1)\n",
    "            num_threads = config.get(\"num_threads\", 4)\n",
    "            \n",
    "            if Loss_type == \"Wasserstein\":\n",
    "                self.using_tf_idf = True\n",
    "                \n",
    "            if self.using_tf_idf:\n",
    "                tf_idf_table = build_tf_idf_table(\n",
    "                    config[\"tgt_vocab_path\"], \n",
    "                    self.src_vocab_size, \n",
    "                    config[\"domain_numb\"], \n",
    "                    config[\"training_feature_file\"])           \n",
    "                self.tf_idf_table = tf_idf_table\n",
    "                \n",
    "            iterator = load_data(\n",
    "                config[\"training_label_file\"], \n",
    "                src_vocab, \n",
    "                batch_size = train_batch_size, \n",
    "                batch_type=config[\"training_batch_type\"], \n",
    "                batch_multiplier = batch_multiplier, \n",
    "                tgt_path=config[\"training_feature_file\"], \n",
    "                tgt_vocab=tgt_vocab, \n",
    "                max_len = max_len, \n",
    "                mode=mode, \n",
    "                shuffle_buffer_size = config[\"sample_buffer_size\"], \n",
    "                num_threads = num_threads, \n",
    "                version = load_data_version, \n",
    "                distribution = example_sampling_distribution\n",
    "            )\n",
    "            \n",
    "            inputs = iterator.get_next()\n",
    "            data_shards = dispatcher.shard(inputs)\n",
    "\n",
    "            with tf.variable_scope(config[\"Architecture\"], initializer=self._initializer(config)):\n",
    "                losses_shards = dispatcher(_loss_op, data_shards, config, mode)\n",
    "\n",
    "            self.loss = _extract_loss(losses_shards, Loss_type=Loss_type) \n",
    "\n",
    "        elif mode == \"Inference\": \n",
    "            assert test_feature_file != None\n",
    "            \n",
    "            iterator = load_data(\n",
    "                test_feature_file, \n",
    "                src_vocab, \n",
    "                batch_size = eval_batch_size, \n",
    "                batch_type = \"examples\", \n",
    "                batch_multiplier = 1, \n",
    "                max_len = max_len, \n",
    "                mode = mode, \n",
    "                version = load_data_version\n",
    "            )\n",
    "            \n",
    "            inputs = iterator.get_next() \n",
    "            \n",
    "            with tf.variable_scope(config[\"Architecture\"]):\n",
    "                _ , self.predictions, _, _, _, _ = self._build(inputs, config, mode)\n",
    "            \n",
    "        self.iterator = iterator\n",
    "        self.inputs = inputs\n",
    "        \n",
    "    def loss_(self):\n",
    "        return self.loss\n",
    "    \n",
    "    def prediction_(self):\n",
    "        return self.predictions\n",
    "   \n",
    "    def inputs_(self):\n",
    "        return self.inputs\n",
    "    \n",
    "    def iterator_initializers(self):\n",
    "        if isinstance(self.iterator,list):\n",
    "            return [iterator.initializer for iterator in self.iterator]\n",
    "        else:\n",
    "            return [self.iterator.initializer]        \n",
    "           \n",
    "    def _build(self, inputs, config, mode):        \n",
    "\n",
    "        debugging = config.get(\"debugging\", False)\n",
    "        Loss_type = self.Loss_type       \n",
    "        print(\"Loss_type: \", Loss_type)           \n",
    "\n",
    "        hidden_size = config[\"hidden_size\"]       \n",
    "        print(\"hidden size: \", hidden_size)\n",
    "                \n",
    "        tgt_vocab_rev = tf.contrib.lookup.index_to_string_table_from_file(config[\"tgt_vocab_path\"], vocab_size= int(self.tgt_vocab_size) - 1, default_value=constants.UNKNOWN_TOKEN)\n",
    "        \n",
    "        end_token = constants.END_OF_SENTENCE_ID\n",
    "        \n",
    "        # Embedding        \n",
    "        size_src = config.get(\"src_embedding_size\",512)\n",
    "        size_tgt = config.get(\"tgt_embedding_size\",512)\n",
    "        latent_variable_size = self.latent_variable_size\n",
    "        \n",
    "        with tf.variable_scope(\"src_embedding\"):\n",
    "            src_emb = create_embeddings(self.src_vocab_size, depth=size_src)\n",
    "\n",
    "        with tf.variable_scope(\"tgt_embedding\"):\n",
    "            tgt_emb = create_embeddings(self.tgt_vocab_size, depth=size_tgt)\n",
    "\n",
    "        self.tgt_emb = tgt_emb\n",
    "        self.src_emb = src_emb\n",
    "\n",
    "        # Build encoder, decoder\n",
    "#---------------------------------------------GRU-----------------------------------------#\n",
    "        if config[\"Architecture\"] == \"GRU\":\n",
    "            nlayers = config.get(\"nlayers\",4)\n",
    "            \n",
    "#==============================ENCODER==================================\n",
    "            encoder = onmt.encoders.BidirectionalRNNEncoder(\n",
    "                nlayers, \n",
    "                hidden_size, \n",
    "                reducer=onmt.layers.ConcatReducer(), \n",
    "                cell_class = tf.contrib.rnn.GRUCell, \n",
    "                dropout=0.1, \n",
    "                residual_connections=True\n",
    "            )\n",
    "#==============================DECODER==================================\n",
    "            decoder = onmt.decoders.AttentionalRNNDecoder(\n",
    "                nlayers, \n",
    "                hidden_size, \n",
    "                bridge=onmt.layers.CopyBridge(), \n",
    "                cell_class=tf.contrib.rnn.GRUCell, \n",
    "                dropout=0.1, \n",
    "                residual_connections=True\n",
    "            )\n",
    "    \n",
    "#---------------------------------------------LSTM-----------------------------------------# \n",
    "        elif config[\"Architecture\"] == \"LSTM\":\n",
    "            nlayers = config.get(\"nlayers\",4)\n",
    "            \n",
    "            assert hidden_size % 2 == 0, \\\n",
    "                \"To use BidirectionalRNN, hidden_size must be devided by 2.\"\n",
    "\n",
    "#==============================ENCODER==================================\n",
    "            encoder_src = onmt.encoders.BidirectionalRNNEncoder(\n",
    "                nlayers, \n",
    "                num_units=hidden_size, \n",
    "                reducer=onmt.layers.ConcatReducer(), \n",
    "                cell_class=tf.nn.rnn_cell.LSTMCell,\n",
    "                dropout=0.1, \n",
    "                residual_connections=True\n",
    "            )\n",
    "        \n",
    "#==============================DECODER==================================\n",
    "            decoder = onmt.decoders.AttentionalRNNDecoder(\n",
    "                nlayers, \n",
    "                num_units=hidden_size, \n",
    "                bridge=onmt.layers.CopyBridge(), \n",
    "                attention_mechanism_class=tf.contrib.seq2seq.LuongAttention,\n",
    "                cell_class=tf.nn.rnn_cell.LSTMCell, \n",
    "                dropout=0.1, \n",
    "                residual_connections=True\n",
    "            )\n",
    "    \n",
    "#---------------------------------------------TRANSFORMER-----------------------------------------#\n",
    "        elif config[\"Architecture\"] == \"Transformer\":\n",
    "            nlayers = config.get(\"nlayers\",6)\n",
    "\n",
    "#==============================ENCODER==================================\n",
    "# Requires 2 encoder for SVAE\n",
    "            encoder_src = onmt.encoders.self_attention_encoder.SelfAttentionEncoder(\n",
    "                nlayers, \n",
    "                num_units=hidden_size, \n",
    "                num_heads=8, \n",
    "                ffn_inner_dim=2048, \n",
    "                dropout=0.1, \n",
    "                attention_dropout=0.1, \n",
    "                relu_dropout=0.1\n",
    "            )  \n",
    "            \n",
    "            encoder_tgt = onmt.encoders.self_attention_encoder.SelfAttentionEncoder(\n",
    "                nlayers, \n",
    "                num_units=hidden_size, \n",
    "                num_heads=8, \n",
    "                ffn_inner_dim=2048, \n",
    "                dropout=0.1, \n",
    "                attention_dropout=0.1, \n",
    "                relu_dropout=0.1\n",
    "            )\n",
    "#==============================DECODER==================================\n",
    "            decoder = onmt.decoders.self_attention_decoder.SelfAttentionDecoder(\n",
    "                nlayers, \n",
    "                num_units=hidden_size, \n",
    "                num_heads=8, \n",
    "                ffn_inner_dim=2048, \n",
    "                dropout=0.1, \n",
    "                attention_dropout=0.1, \n",
    "                relu_dropout=0.1\n",
    "            )\n",
    "\n",
    "        print(\"Model type: \", config[\"Architecture\"])\n",
    "        \n",
    "        output_layer = None\n",
    "\n",
    "#-----------------------------------------------------TRAINING MODE-----------------------------------------------#\n",
    "        if mode ==\"Training\":            \n",
    "            print(\"Building model in Training mode\")\n",
    "            \n",
    "            src_length = inputs[\"src_length\"]\n",
    "            tgt_length = inputs[\"tgt_length\"]\n",
    "            \n",
    "            emb_src_batch = tf.nn.embedding_lookup(src_emb, inputs[\"src_ids\"]) # dim = [batch, length, depth]\n",
    "            emb_tgt_batch = tf.nn.embedding_lookup(tgt_emb, inputs[\"tgt_ids\"])  \n",
    "            emb_tgt_batch_in = tf.nn.embedding_lookup(tgt_emb, inputs[\"tgt_ids_in\"])   \n",
    "            \n",
    "            self.emb_tgt_batch = emb_tgt_batch\n",
    "            self.emb_src_batch = emb_src_batch\n",
    "            \n",
    "            print(\"emb_src_batch: \", emb_src_batch)\n",
    "            print(\"emb_tgt_batch: \", emb_tgt_batch)\n",
    "            \n",
    "            tgt_ids_batch = inputs[\"tgt_ids_out\"]\n",
    "                        \n",
    "            #========ENCODER_PROCESS======================\n",
    "            with tf.variable_scope(\"encoder\", reuse=tf.AUTO_REUSE):\n",
    "                \n",
    "                if config[\"Architecture\"] == \"Transformer\":\n",
    "                    \n",
    "                    encoder_output_src = encoder_src.encode(\n",
    "                        emb_src_batch, \n",
    "                        sequence_length = src_length, \n",
    "                        mode=tf.estimator.ModeKeys.TRAIN\n",
    "                    )\n",
    "\n",
    "                    encoder_output_tgt = encoder_tgt.encode(\n",
    "                        emb_tgt_batch, \n",
    "                        sequence_length = tgt_length, \n",
    "                        mode=tf.estimator.ModeKeys.TRAIN\n",
    "                    )\n",
    "\n",
    "                    self.encoder_output_src = encoder_output_src\n",
    "                    self.encoder_output_tgt = encoder_output_tgt\n",
    "\n",
    "                    encoder_outputs_src, encoder_states_src, encoder_seq_length_src = encoder_output_src\n",
    "                    encoder_outputs_tgt, encoder_states_tgt, _ = encoder_output_tgt\n",
    "                        # encoder_outputs: [batch_size, max_length, hidden_units_size]\n",
    "                        # encoder_states: nlayers ° [batch_size, hidden_units_size] (tuple of 2d array)\n",
    "\n",
    "                    encoder_states_combined = tf.concat([encoder_states_src[-1], encoder_states_tgt[-1]], 1)\n",
    "                        # dim = [batch_size, hidden_size + hidden_size]\n",
    "                        \n",
    "                elif config[\"Architecture\"] == \"LSTM\" :\n",
    "                    \n",
    "                    encoder_output_src = encoder_src.encode(\n",
    "                        emb_src_batch, \n",
    "                        sequence_length = src_length, \n",
    "                        mode=tf.estimator.ModeKeys.TRAIN\n",
    "                    )\n",
    "                    \n",
    "                    encoder_outputs_src, encoder_states_src, encoder_seq_length_src = encoder_output_src\n",
    "                                        \n",
    "                    encoder_states_combined = tf.reshape(tf.transpose(encoder_states_src, perm=[2, 0, 1, 3]), [tf.shape(encoder_outputs_src)[0], -1])\n",
    "                        # dim = [batch_size, 2 * nlayers * hidden_size]\n",
    "                    print(\"encoder_states_combined\",encoder_states_combined)\n",
    "\n",
    "                # important infos\n",
    "                batch_size = tf.shape(encoder_outputs_src)[0]\n",
    "                max_length = tf.shape(encoder_outputs_src)[1]\n",
    "                    \n",
    "            #======GENERATIVE_PROCESS====================\n",
    "            \n",
    "            with tf.variable_scope(\"generator\"):\n",
    "                \n",
    "                if config[\"Architecture\"] == \"Transformer\":\n",
    "                    input_latent_size = 2 * hidden_size\n",
    "                        # 2 hidden states from source and target sentences\n",
    "                else:\n",
    "                    input_latent_size = 2 * nlayers * hidden_size\n",
    "                        # in a RNN model, the hidden states from source is passed into target's encoder process\n",
    "                        # still *2 if use a Bidirectional RNN model\n",
    "                        \n",
    "\n",
    "                W_out_to_mu = tf.get_variable('output_to_mu_weight', shape = [input_latent_size, latent_variable_size])\n",
    "                b_out_to_mu = tf.get_variable('output_to_mu_bias', shape = [latent_variable_size])\n",
    "              \n",
    "                W_out_to_logvar = tf.get_variable('output_to_logvar_weight', shape = [input_latent_size, latent_variable_size])\n",
    "                b_out_to_logvar = tf.get_variable('output_to_logvar_bias', shape = [latent_variable_size])\n",
    "\n",
    "                mu = tf.nn.sigmoid(tf.add(tf.matmul(encoder_states_combined, W_out_to_mu), b_out_to_mu))\n",
    "                logvar = tf.nn.sigmoid(tf.add(tf.matmul(encoder_states_combined, W_out_to_logvar), b_out_to_logvar))\n",
    "            \n",
    "                std = tf.exp(0.5 * logvar)\n",
    "\n",
    "                z = tf.random_normal([batch_size, latent_variable_size])\n",
    "                    # z, mu, logvar shape [batch_size, latent_size]\n",
    "                z = z * std + mu \n",
    "                    #Rappel z: [batch_size, latent_size]\n",
    "                    \n",
    "                if config[\"Architecture\"] == \"Transformer\":\n",
    "                    \n",
    "                    z = tf.reshape(z, [batch_size,1,-1]) \n",
    "                        # shape [batch_size, 1, latent_size]\n",
    "                    \n",
    "                    zz = tf.tile(z,[1, max_length, 1])\n",
    "                        # shape [batch_size, max_length, latent_size]\n",
    "\n",
    "                    new_memory_inputs = tf.concat([encoder_outputs_src, zz], 2)\n",
    "\n",
    "                    new_memory_inputs = tf.reshape(new_memory_inputs,[batch_size, -1, hidden_size + latent_variable_size])\n",
    "\n",
    "                    new_encoder_states_src = encoder_states_src\n",
    "                    \n",
    "                    print(\"encoder_outputs_src\",encoder_outputs_src)\n",
    "                    print(\"new_memory_inputs\",new_memory_inputs)\n",
    "                    \n",
    "                elif config[\"Architecture\"] == \"LSTM\":\n",
    "\n",
    "                    new_memory_inputs = encoder_outputs_src\n",
    "                    \n",
    "                    W_z_to_state = tf.get_variable('z_to_state_weight', shape = [latent_variable_size, input_latent_size])\n",
    "                    b_z_to_state = tf.get_variable('z_to_state_bias', shape = [input_latent_size])\n",
    "                    states_from_z = tf.add(tf.matmul(z, W_z_to_state), b_z_to_state)\n",
    "                    \n",
    "                    t_states_rebuild = tf.transpose(tf.reshape(states_from_z, [batch_size, nlayers, 2, -1]), perm=[1,2,0,3])\n",
    "                    \n",
    "                    t_states_rebuild = tf.unstack(t_states_rebuild)\n",
    "                                                            \n",
    "                    new_encoder_states_src = tuple([tf.nn.rnn_cell.LSTMStateTuple\n",
    "                                                    (tf.reshape(t_states_rebuild[i][0],[batch_size, hidden_size]),\n",
    "                                                    tf.reshape(t_states_rebuild[i][1],[batch_size, hidden_size])) \n",
    "                                                                                            for i in range(nlayers)]\n",
    "                                                  )\n",
    "\n",
    "                    print(\"encoder_outputs_src\",encoder_outputs_src)\n",
    "                    print(\"new_memory_inputs\",new_memory_inputs)\n",
    "               \n",
    "            #======DECODER_PROCESS====================\n",
    "            with tf.variable_scope(\"decoder\"): \n",
    "\n",
    "                logits, dec_states, dec_length, attention = decoder.decode(\n",
    "                                          emb_tgt_batch_in, \n",
    "                                          tgt_length + 1,\n",
    "                                          vocab_size = int(self.tgt_vocab_size),\n",
    "                                          initial_state = new_encoder_states_src,\n",
    "                                          output_layer = output_layer,                                              \n",
    "                                          mode = tf.estimator.ModeKeys.TRAIN,\n",
    "                                          memory = new_memory_inputs,\n",
    "                                          memory_sequence_length = encoder_seq_length_src,\n",
    "                                          return_alignment_history = True\n",
    "                )\n",
    "                \n",
    "                outputs = {\n",
    "                        \"logits\": logits\n",
    "                        }\n",
    "                \n",
    "            predictions = None\n",
    "\n",
    "#-----------------------------------------------------INFERENCE MODE-----------------------------------------------#\n",
    "        elif mode == \"Inference\":\n",
    "            \n",
    "            print(\"Build model in Inference mode\")\n",
    "            \n",
    "            beam_width = config.get(\"beam_width\", 5)\n",
    "            \n",
    "            src_length = inputs[\"src_length\"]\n",
    "            emb_src_batch = tf.nn.embedding_lookup(src_emb, inputs[\"src_ids\"])\n",
    "                        \n",
    "            start_tokens = tf.fill([tf.shape(inputs[\"src_ids\"])[0]], constants.START_OF_SENTENCE_ID)\n",
    "            \n",
    "           #========ENCODER_PROCESS======================\n",
    "            with tf.variable_scope(\"encoder\", reuse=tf.AUTO_REUSE):\n",
    "                \n",
    "                encoder_output_src = encoder_src.encode(\n",
    "                    emb_src_batch, \n",
    "                    sequence_length = src_length, \n",
    "                    mode=tf.estimator.ModeKeys.TRAIN\n",
    "                )\n",
    "                \n",
    "                encoder_outputs_src, encoder_states_src, encoder_seq_length_src = encoder_output_src\n",
    "                \n",
    "                encoder_states_combined = tf.reshape(tf.transpose(encoder_states_src, perm=[2, 0, 1, 3]), [tf.shape(encoder_outputs_src)[0], -1])\n",
    "                    # dim = [batch_size, 2 * nlayers * hidden_size]\n",
    "                print(\"encoder_states_combined\",encoder_states_combined)\n",
    "                \n",
    "                # important infos\n",
    "                batch_size = tf.shape(encoder_outputs_src)[0]\n",
    "                max_length = tf.shape(encoder_outputs_src)[1]\n",
    "\n",
    "                input_latent_size = 2 * nlayers * hidden_size\n",
    "                 \n",
    "            #======GENERATIVE_PROCESS====================\n",
    "            with tf.variable_scope(\"generator\"):\n",
    "\n",
    "                z = tf.random_normal([batch_size, latent_variable_size])\n",
    "                    # z, mu, logvar shape [batch_size, latent_size]\n",
    "                \n",
    "                # Inference from normale\n",
    "                std_0 = tf.exp(0.5 * self.logvar_0)\n",
    "\n",
    "                z_0 = z* std_0 + self.mu_0 \n",
    "                \n",
    "                # Inference from input\n",
    "                    \n",
    "                W_out_to_mu = tf.get_variable('output_to_mu_weight', shape = [input_latent_size, latent_variable_size])\n",
    "                b_out_to_mu = tf.get_variable('output_to_mu_bias', shape = [latent_variable_size])\n",
    "\n",
    "                W_out_to_logvar = tf.get_variable('output_to_logvar_weight', shape = [input_latent_size, latent_variable_size])\n",
    "                b_out_to_logvar = tf.get_variable('output_to_logvar_bias', shape = [latent_variable_size])\n",
    "\n",
    "                mu = tf.nn.sigmoid(tf.add(tf.matmul(encoder_states_combined, W_out_to_mu), b_out_to_mu))\n",
    "                logvar = tf.nn.sigmoid(tf.add(tf.matmul(encoder_states_combined, W_out_to_logvar), b_out_to_logvar))\n",
    "                std = tf.exp(0.5 * logvar)\n",
    "\n",
    "                z_1 = z * std + mu \n",
    "                    \n",
    "                if config[\"Architecture\"] == \"Transformer\":\n",
    "                    \n",
    "                    z = tf.reshape(z, [batch_size,1,-1]) \n",
    "                        # shape [batch_size, 1, latent_size]\n",
    "                    \n",
    "                    zz = tf.tile(z,[1, max_length, 1])\n",
    "                        # shape [batch_size, max_length, latent_size]\n",
    "\n",
    "                    new_memory_inputs = tf.concat([encoder_outputs_src, zz], 2)\n",
    "\n",
    "                    new_memory_inputs = tf.reshape(new_memory_inputs,[batch_size, -1, hidden_size + latent_variable_size])\n",
    "\n",
    "                    new_encoder_states_src = encoder_states_src\n",
    "                    \n",
    "                elif config[\"Architecture\"] == \"LSTM\":\n",
    "                    \n",
    "                    W_z_to_state = tf.get_variable('z_to_state_weight', shape = [latent_variable_size, input_latent_size])\n",
    "                    b_z_to_state = tf.get_variable('z_to_state_bias', shape = [input_latent_size])\n",
    "                    \n",
    "                    # Inference from normale\n",
    "                    states_from_z_0 = tf.add(tf.matmul(z_0, W_z_to_state), b_z_to_state)\n",
    "                    t_states_rebuild = tf.transpose(tf.reshape(states_from_z_0, [batch_size, nlayers, 2, -1]), perm=[1,2,0,3])\n",
    "                    t_states_rebuild = tf.unstack(t_states_rebuild)\n",
    "                    \n",
    "                    new_encoder_states_src_0 = tuple([tf.nn.rnn_cell.LSTMStateTuple\n",
    "                                                    (tf.reshape(t_states_rebuild[i][0],[batch_size, hidden_size]),\n",
    "                                                    tf.reshape(t_states_rebuild[i][1],[batch_size, hidden_size])) \n",
    "                                                                                            for i in range(nlayers)]\n",
    "                                                  )\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    # Inference from input\n",
    "                    states_from_z_1 = tf.add(tf.matmul(z_1, W_z_to_state), b_z_to_state)\n",
    "                    t_states_rebuild = tf.transpose(tf.reshape(states_from_z_1, [batch_size, nlayers, 2, -1]), perm=[1,2,0,3])\n",
    "                    t_states_rebuild = tf.unstack(t_states_rebuild)\n",
    "                    \n",
    "                    new_encoder_states_src_1 = tuple([tf.nn.rnn_cell.LSTMStateTuple\n",
    "                                                    (tf.reshape(t_states_rebuild[i][0],[batch_size, hidden_size]),\n",
    "                                                    tf.reshape(t_states_rebuild[i][1],[batch_size, hidden_size])) \n",
    "                                                                                            for i in range(nlayers)]\n",
    "                                                  )\n",
    "                    \n",
    "                    new_memory_inputs = encoder_outputs_src\n",
    "            \n",
    "            #======DECODER_PROCESS====================\n",
    "                    \n",
    "                \n",
    "            print(\"Inference with beam width %d\"%(beam_width))\n",
    "            maximum_iterations = config.get(\"maximum_iterations\", tf.round(2 * max_length))\n",
    "\n",
    "            if beam_width <= 1:  \n",
    "                \n",
    "                with tf.variable_scope(\"decoder\"):\n",
    "                    sampled_ids_0, _, sampled_length_0, log_probs_0, alignment_0 = decoder.dynamic_decode(\n",
    "                                                                    tgt_emb,\n",
    "                                                                    start_tokens,\n",
    "                                                                    end_token,\n",
    "                                                                    vocab_size=int(self.tgt_vocab_size),\n",
    "                                                                    initial_state=new_encoder_states_src_0,\n",
    "                                                                    maximum_iterations=maximum_iterations,\n",
    "                                                                    output_layer = output_layer,\n",
    "                                                                    mode=tf.estimator.ModeKeys.PREDICT,\n",
    "                                                                    memory=new_memory_inputs,\n",
    "                                                                    memory_sequence_length=encoder_seq_length_src,\n",
    "                                                                    dtype=tf.float32,\n",
    "                                                                    return_alignment_history=True\n",
    "                                                                    )\n",
    "                with tf.variable_scope(\"decoder\", reuse=True): \n",
    "                    sampled_ids_1, _, sampled_length_1, log_probs_1, alignment_1 = decoder.dynamic_decode(\n",
    "                                                                    tgt_emb,\n",
    "                                                                    start_tokens,\n",
    "                                                                    end_token,\n",
    "                                                                    vocab_size=int(self.tgt_vocab_size),\n",
    "                                                                    initial_state=new_encoder_states_src_1,\n",
    "                                                                    maximum_iterations=maximum_iterations,\n",
    "                                                                    output_layer = output_layer,\n",
    "                                                                    mode=tf.estimator.ModeKeys.PREDICT,\n",
    "                                                                    memory=new_memory_inputs,\n",
    "                                                                    memory_sequence_length=encoder_seq_length_src,\n",
    "                                                                    dtype=tf.float32,\n",
    "                                                                    return_alignment_history=True\n",
    "                                                                    )\n",
    "            else:\n",
    "                length_penalty = config.get(\"length_penalty\", 0)\n",
    "                \n",
    "                with tf.variable_scope(\"decoder\"):\n",
    "                    sampled_ids_0, _, sampled_length_0, log_probs_0, alignment_0 = decoder.dynamic_decode_and_search(\n",
    "                                                            tgt_emb,\n",
    "                                                            start_tokens,\n",
    "                                                            end_token,\n",
    "                                                            vocab_size = int(self.tgt_vocab_size),\n",
    "                                                            initial_state = new_encoder_states_src_0,\n",
    "                                                            beam_width = beam_width,\n",
    "                                                            length_penalty = length_penalty,\n",
    "                                                            maximum_iterations = maximum_iterations,\n",
    "                                                            output_layer = output_layer,\n",
    "                                                            mode = tf.estimator.ModeKeys.PREDICT,\n",
    "                                                            memory = new_memory_inputs,\n",
    "                                                            memory_sequence_length = encoder_seq_length_src,\n",
    "                                                            dtype=tf.float32,\n",
    "                                                            return_alignment_history = True)\n",
    "                with tf.variable_scope(\"decoder\", reuse=True): \n",
    "                    sampled_ids_1, _, sampled_length_1, log_probs_1, alignment_1 = decoder.dynamic_decode_and_search(\n",
    "                                                            tgt_emb,\n",
    "                                                            start_tokens,\n",
    "                                                            end_token,\n",
    "                                                            vocab_size = int(self.tgt_vocab_size),\n",
    "                                                            initial_state = new_encoder_states_src_1,\n",
    "                                                            beam_width = beam_width,\n",
    "                                                            length_penalty = length_penalty,\n",
    "                                                            maximum_iterations = maximum_iterations,\n",
    "                                                            output_layer = output_layer,\n",
    "                                                            mode = tf.estimator.ModeKeys.PREDICT,\n",
    "                                                            memory = new_memory_inputs,\n",
    "                                                            memory_sequence_length = encoder_seq_length_src,\n",
    "                                                            dtype=tf.float32,\n",
    "                                                            return_alignment_history = True)\n",
    "\n",
    "            target_tokens_0 = tgt_vocab_rev.lookup(tf.cast(sampled_ids_0, tf.int64))\n",
    "            target_tokens_1 = tgt_vocab_rev.lookup(tf.cast(sampled_ids_1, tf.int64))\n",
    "            \n",
    "            predictions = [\n",
    "                {\n",
    "              \"tokens\": target_tokens_0,\n",
    "              \"length\": sampled_length_0,\n",
    "              \"log_probs\": log_probs_0,\n",
    "              \"alignment\": alignment_0,\n",
    "                            },\n",
    "                {\n",
    "              \"tokens\": target_tokens_1,\n",
    "              \"length\": sampled_length_1,\n",
    "              \"log_probs\": log_probs_1,\n",
    "              \"alignment\": alignment_1,\n",
    "                            } ]\n",
    "            \n",
    "            tgt_ids_batch = None\n",
    "            tgt_length = None\n",
    "            mu = None\n",
    "            logvar = None\n",
    "            outputs = None\n",
    "            \n",
    "        self.outputs = outputs\n",
    "        self.mu = mu\n",
    "        self.logvar = logvar\n",
    "        \n",
    "        return outputs, predictions, tgt_ids_batch, tgt_length, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(config_file, checkpoint_path=None, test_feature_file=None):\n",
    "    \n",
    "    with open(config_file, \"r\") as stream:\n",
    "        config = yaml.load(stream)\n",
    "        \n",
    "    assert test_feature_file!=None\n",
    "    \n",
    "    from opennmt.utils.misc import print_bytes\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with tf.Session(graph=graph,config=tf.ConfigProto(log_device_placement=False, allow_soft_placement=True, gpu_options=tf.GPUOptions(allow_growth=True))) as sess_:\n",
    "     \n",
    "        eval_model = Model(config_file, \"Inference\", test_feature_file)\n",
    "        #emb_src_batch = eval_model.emb_src_batch_()\n",
    "        saver = tf.train.Saver()\n",
    "        tf.tables_initializer().run()\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        if checkpoint_path==None:\n",
    "            checkpoint_dir = config[\"model_dir\"]\n",
    "            checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "        print((\"Evaluating model %s\"%checkpoint_path))\n",
    "        saver.restore(sess_, checkpoint_path)        \n",
    "\n",
    "        predictions_0, predictions_1 = eval_model.prediction_()\n",
    "        \n",
    "        tokens_0 = predictions_0[\"tokens\"]\n",
    "        length_0 = predictions_0[\"length\"]    \n",
    "        \n",
    "        tokens_1 = predictions_1[\"tokens\"]\n",
    "        length_1 = predictions_1[\"length\"]\n",
    "        \n",
    "        sess_.run(eval_model.iterator_initializers())\n",
    "        \n",
    "        # pred_dict = sess_.run([predictions])\n",
    "        pred_dict = None\n",
    "        \n",
    "        print(\"write to :%s\"%os.path.join(config[\"model_dir\"],\"eval\",os.path.basename(test_feature_file) + \".trans.\" + os.path.basename(checkpoint_path)))\n",
    "        \n",
    "        normal_eval_path = os.path.join(config[\"model_dir\"],\"eval\",os.path.basename(test_feature_file) + \".trans.\" + os.path.basename(checkpoint_path) + \".normal\")\n",
    "        from_input_eval_path = os.path.join(config[\"model_dir\"],\"eval\",os.path.basename(test_feature_file) + \".trans.\" + os.path.basename(checkpoint_path) + \".frominput\")\n",
    "        \n",
    "        with open(normal_eval_path,\"w\") as output_0_, \\\n",
    "            open(from_input_eval_path,\"w\") as output_1_:\n",
    "            while True:                 \n",
    "                try:                \n",
    "                    _tokens_1, _length_1, _tokens_0, _length_0 = sess_.run([tokens_1, length_1, tokens_0, length_0])                    \n",
    "                    #print emb_src_batch_\n",
    "                    for b in range(_tokens_0.shape[0]):                        \n",
    "                        pred_toks = _tokens_0[b][0][:_length_0[b][0] - 1]                                                \n",
    "                        pred_sent = b\" \".join(pred_toks)                        \n",
    "                        print_bytes(pred_sent, output_0_)    \n",
    "                        \n",
    "                    for b in range(_tokens_1.shape[0]):                        \n",
    "                        pred_toks = _tokens_1[b][0][:_length_1[b][0] - 1]                                                \n",
    "                        pred_sent = b\" \".join(pred_toks)                        \n",
    "                        print_bytes(pred_sent, output_1_)   \n",
    "                        \n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "        \n",
    "    return normal_eval_path, from_input_eval_path, pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config_file = 'config/SVAE_zS_config_tuanh_WMT.yml'\n",
    "\n",
    "with open(config_file, \"r\") as stream:\n",
    "    config = yaml.load(stream)\n",
    "    \n",
    "# Eval directory stores prediction files\n",
    "if not os.path.exists(os.path.join(config[\"model_dir\"],\"eval\")):\n",
    "    os.makedirs(os.path.join(config[\"model_dir\"],\"eval\"))\n",
    "if not os.path.exists(os.path.join(config[\"model_dir\"],\"important ckpts\")):\n",
    "    os.makedirs(os.path.join(config[\"model_dir\"],\"important ckpts\"))\n",
    "    \n",
    "training_model = Model(config_file, \"Training\")\n",
    "\n",
    "global_step = tf.train.create_global_step()\n",
    "\n",
    "if config.get(\"Loss_Function\",\"Cross_Entropy\")==\"Cross_Entropy\":\n",
    "    loss, kl_loss = training_model.loss_()\n",
    "    use_kl_weight = config.get(\"use_kl_weight\", True)\n",
    "    # use_kl_weight = False\n",
    "    if use_kl_weight : \n",
    "        kl_weight = tf.cond(loss > 2., lambda: tf.minimum(kl_coeff(global_step),0.001), lambda: tf.minimum(kl_coeff(global_step), 1))\n",
    "        kl_weight = tf.cond(kl_loss > 1., lambda: kl_weight, lambda: tf.minimum(kl_coeff(global_step),0.001))\n",
    "    else:\n",
    "        kl_weight = tf.constant(1)\n",
    "\n",
    "    tf.summary.scalar(\"kl_weight\", kl_weight)\n",
    "    generator_total_loss = loss + kl_loss * kl_weight\n",
    "\n",
    "inputs = training_model.inputs_()\n",
    "\n",
    "if config[\"mode\"] == \"Training\":\n",
    "    optimizer_params = config[\"optimizer_parameters\"]\n",
    "    with tf.variable_scope(\"main_training\"):\n",
    "        train_op, accum_vars_ = optimize_loss(generator_total_loss, config[\"optimizer_parameters\"])\n",
    "        \n",
    "Eval_dataset_numb = len(config[\"eval_label_file\"])\n",
    "print(\"Number of validation set: \", Eval_dataset_numb)\n",
    "external_evaluator = [None] * Eval_dataset_numb \n",
    "writer_bleu = [None] * Eval_dataset_numb \n",
    "print(1)\n",
    "for i in range(Eval_dataset_numb):\n",
    "    external_evaluator[i] = BLEUEvaluator(config[\"eval_label_file\"][i], config[\"model_dir\"])\n",
    "    writer_bleu[i] = [tf.summary.FileWriter(os.path.join(config[\"model_dir\"],\"BLEU\",\"domain_%d\"%i,\"from_normal\")),\n",
    "                      tf.summary.FileWriter(os.path.join(config[\"model_dir\"],\"BLEU\",\"domain_%d\"%i,\"from_input\"))]\n",
    "print(2)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False, allow_soft_placement=True, gpu_options=tf.GPUOptions(allow_growth=True)))\n",
    "print(3)\n",
    "writer = tf.summary.FileWriter(config[\"model_dir\"])\n",
    "print(4)\n",
    "var_list_ = tf.global_variables()\n",
    "print(5)\n",
    "saver = tf.train.Saver(var_list_, max_to_keep=config[\"max_to_keep\"])\n",
    "saver_max0 = tf.train.Saver(var_list_, max_to_keep = 1)\n",
    "saver_max1 = tf.train.Saver(var_list_, max_to_keep = 1)\n",
    "saver_maxkld = tf.train.Saver(var_list_, max_to_keep = 1)\n",
    "checkpoint_path = tf.train.latest_checkpoint(config[\"model_dir\"])\n",
    "print(6)\n",
    "sess.run(global_step.initializer)\n",
    "print(6.35)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(7)\n",
    "training_summary = tf.summary.merge_all()\n",
    "global_step_ = sess.run(global_step)\n",
    "print(8)\n",
    "if checkpoint_path:\n",
    "    try :\n",
    "        print(\"Continue training:...\")\n",
    "        print(\"Load parameters from %s\"%checkpoint_path)\n",
    "        saver.restore(sess, checkpoint_path)        \n",
    "        global_step_ = sess.run(global_step)\n",
    "        print(\"global_step: \", global_step_)\n",
    "\n",
    "        for i in range(Eval_dataset_numb):\n",
    "            normal_eval_path, from_input_eval_path, prediction_dict = inference(config_file, checkpoint_path, config[\"eval_feature_file\"][i])\n",
    "            score_0 = external_evaluator[i].score(config[\"eval_label_file\"][i], normal_eval_path)\n",
    "            score_1 = external_evaluator[i].score(config[\"eval_label_file\"][i], from_input_eval_path)\n",
    "            print(\"=========================================EVALUATION SCORE==========================================\")\n",
    "            print(\"From Normal BLEU at checkpoint %s for testset %s: %f\"%(checkpoint_path, config[\"eval_feature_file\"][i], score_0))\n",
    "            print(\"From Input BLEU at checkpoint %s for testset %s: %f\"%(checkpoint_path, config[\"eval_feature_file\"][i], score_1))\n",
    "            print(\"=====================================================================================\")\n",
    "    except TypeError:\n",
    "        print(\"There is a TypeError, the output maybe empty !\")\n",
    "        pass\n",
    "\n",
    "else:\n",
    "    print(\"Training from scratch\")\n",
    "    \n",
    "sess.run(tf.tables_initializer())\n",
    "sess.run(training_model.iterator_initializers())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = []\n",
    "best_bleu_0 = 0.\n",
    "best_bleu_1 = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_time = 0.\n",
    "\n",
    "print(\"Start training from step {:d}...\".format(global_step_))\n",
    "\n",
    "while global_step_ <= config[\"iteration_number\"]:                       \n",
    "\n",
    "    #=================== 1 iteration=======================\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if use_kl_weight:\n",
    "        ce_loss_, kl_loss_,loss_, global_step_, _, kl_weight_ = sess.run([loss, kl_loss, generator_total_loss, global_step, train_op, kl_weight])     \n",
    "    else:\n",
    "        ce_loss_, kl_loss_,loss_, global_step_, _ = sess.run([loss, kl_loss, generator_total_loss, global_step, train_op])     \n",
    "        kl_weight_ = 1\n",
    "    \n",
    "    run_time += time.time() - start_time\n",
    "    \n",
    "    total_loss.append(loss_)\n",
    "\n",
    "    #==================printing things======================\n",
    "    if (np.mod(global_step_, config[\"printing_freq\"])) == 0:            \n",
    "        print((datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "        print(\"*************Step: {:d} - RTime: {:f}***************\".format(global_step_,run_time))\n",
    "        print(\"CE Loss: {:4f} - KL Loss: {:4f} - KL Weight: {:4f}\".format(ce_loss_, kl_loss_,kl_weight_))\n",
    "        print(\"TotalLoss at step {:d}: {:4f}\".format(global_step_, np.mean(total_loss)))\n",
    "        run_time = 0.             \n",
    "\n",
    "    if (np.mod(global_step_, config[\"summary_freq\"])) == 0:\n",
    "        training_summary_ = sess.run(training_summary)\n",
    "        writer.add_summary(training_summary_, global_step=global_step_)\n",
    "        writer.flush()\n",
    "        total_loss = []\n",
    "\n",
    "    if (np.mod(global_step_, config[\"save_freq\"])) == 0 and global_step_ > 0:    \n",
    "        print((datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "        checkpoint_path = os.path.join(config[\"model_dir\"], 'model.ckpt')\n",
    "        print((\"save to %s\"%(checkpoint_path)))\n",
    "        saver.save(sess, checkpoint_path, global_step = global_step_)\n",
    "        \n",
    "        if kl_loss_ > 1.5 :\n",
    "            checkpoint_path = os.path.join(config[\"model_dir\"],\"important ckpts\", 'lastBigKLD.model.ckpt')\n",
    "            saver_maxkld.save(sess, checkpoint_path, global_step = global_step_)\n",
    "\n",
    "    if (np.mod(global_step_, config[\"eval_freq\"])) == 0 and global_step_ >0: \n",
    "        try :\n",
    "            checkpoint_path = tf.train.latest_checkpoint(config[\"model_dir\"])\n",
    "            for i in range(Eval_dataset_numb):\n",
    "                normal_eval_path, from_input_eval_path, prediction_dict = inference(config_file, checkpoint_path, config[\"eval_feature_file\"][i])\n",
    "                score_0 = external_evaluator[i].score(config[\"eval_label_file\"][i], normal_eval_path)\n",
    "                score_1 = external_evaluator[i].score(config[\"eval_label_file\"][i], from_input_eval_path)\n",
    "                \n",
    "                print(\"=========================================EVALUATION SCORE==========================================\")\n",
    "                print(\"From Normal BLEU at checkpoint %s for testset %s: %f\"%(checkpoint_path, config[\"eval_feature_file\"][i], score_0))\n",
    "                print(\"From Input BLEU at checkpoint %s for testset %s: %f\"%(checkpoint_path, config[\"eval_feature_file\"][i], score_1))\n",
    "                print(\"=====================================================================================\")\n",
    "                \n",
    "                if score_0 > best_bleu_0 :\n",
    "                    print((datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "                    checkpoint_path = os.path.join(config[\"model_dir\"],\"important ckpts\", 'bestBLEU_0.model.ckpt')\n",
    "                    print((\"save to %s\"%(checkpoint_path)))\n",
    "                    saver_max0.save(sess, checkpoint_path, global_step = global_step_)\n",
    "                    best_bleu_0 = score_0\n",
    "                    \n",
    "                if score_1 > best_bleu_1 :\n",
    "                    print((datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "                    checkpoint_path = os.path.join(config[\"model_dir\"],\"important ckpts\", 'bestBLEU_1.model.ckpt')\n",
    "                    print((\"save to %s\"%(checkpoint_path)))\n",
    "                    saver_max1.save(sess, checkpoint_path, global_step = global_step_)\n",
    "                    best_bleu_1 = score_1\n",
    "                \n",
    "                score_summary = tf.Summary(value=[tf.Summary.Value(tag=\"eval_score_%d\"%i, simple_value=score_0)])\n",
    "                writer_bleu[i][0].add_summary(score_summary, global_step_)\n",
    "                writer_bleu[i][0].flush()\n",
    "                \n",
    "                score_summary = tf.Summary(value=[tf.Summary.Value(tag=\"eval_score_%d\"%i, simple_value=score_1)])\n",
    "                writer_bleu[i][1].add_summary(score_summary, global_step_)\n",
    "                writer_bleu[i][1].flush()\n",
    "                \n",
    "        except TypeError:\n",
    "            print(\"There is a TypeError, the output maybe empty !\")\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kl_weight = tf.cond(loss > 2., lambda: tf.minimum(kl_coeff(global_step),0.01), lambda: tf.minimum(kl_coeff(global_step), 1))\n",
    "# tf.summary.scalar(\"kl_weight\", kl_weight)\n",
    "# generator_total_loss = loss + kl_loss * kl_weight\n",
    "# training_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_now(config_file,test_feature_file, checkpoint_path=None, num_iter=10):\n",
    "    \n",
    "    with open(config_file, \"r\") as stream:\n",
    "        config = yaml.load(stream)\n",
    "        \n",
    "    if beam_width is not None :\n",
    "        config[\"beam_width\"] = beam_width\n",
    "        \n",
    "    def print_bytes_sentence(byted_list) :\n",
    "        sen = []\n",
    "        for b in byted_list:\n",
    "            sb = b.decode(\"utf-8\") \n",
    "            if sb not in ['<blank>','<s>','</s>'] :\n",
    "                sen.append(sb)\n",
    "        print(\" \".join(sen))\n",
    "        return None\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with tf.Session(graph=graph,config=tf.ConfigProto(log_device_placement=False, allow_soft_placement=True, gpu_options=tf.GPUOptions(allow_growth=True))) as sess_:\n",
    "     \n",
    "        eval_model = Model(config_file, \"Inference\", test_feature_file)\n",
    "        #emb_src_batch = eval_model.emb_src_batch_()\n",
    "        saver = tf.train.Saver()\n",
    "        tf.tables_initializer().run()\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        if checkpoint_path==None:\n",
    "            checkpoint_dir = config[\"model_dir\"]\n",
    "            checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "        print((\"Evaluating model %s\"%checkpoint_path))\n",
    "        saver.restore(sess_, checkpoint_path)        \n",
    "\n",
    "        _,predictions = eval_model.prediction_()\n",
    "        \n",
    "        tokens = predictions[\"tokens\"]\n",
    "        length = predictions[\"length\"]            \n",
    "        \n",
    "        for _ in range(num_iter) :\n",
    "            \n",
    "            sess_.run(eval_model.iterator_initializers())\n",
    "\n",
    "            preds = sess_.run(tokens)\n",
    "\n",
    "            with open(test_feature_file) as fp:  \n",
    "                lines = fp.readlines()\n",
    "\n",
    "            for i in range(len(lines)):\n",
    "                print(\"----------------------------------\")\n",
    "                print(\"Source sentence: \")\n",
    "                print(lines[i])\n",
    "                print(\"Generated sentences:\")\n",
    "                for tokenized_line in preds[i]:\n",
    "                    print_bytes_sentence(tokenized_line)\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions = [\n",
    "    'What is the best start up idea ? \\n'\n",
    "]\n",
    "\n",
    "with open('infer_test', 'w') as file_:\n",
    "    file_.writelines(Questions)\n",
    "    \n",
    "inference_now(config_file, test_feature_file='infer_test', checkpoint_path=None, num_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3.6]",
   "language": "python",
   "name": "conda-env-py3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
